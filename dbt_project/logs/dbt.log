[0m17:28:07.454387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1033c7f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b82550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105b82130>]}


============================== 17:28:07.457307 | 98c74227-1467-4045-b910-02956fe2fe78 ==============================
[0m17:28:07.457307 [info ] [MainThread]: Running with dbt=1.8.7
[0m17:28:07.457671 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/ankurchopra/.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m17:28:07.503852 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:28:07.504235 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:28:07.504424 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:28:08.202112 [error] [MainThread]: Encountered an error:
Runtime Error
  at path ['name']: 'Business Accounts' does not match '^[^\\d\\W]\\w*$'

Error encountered in /Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/dbt_project.yml
[0m17:28:08.204072 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 0.7921536, "process_user_time": 1.491808, "process_kernel_time": 2.33104, "process_mem_max_rss": "199639040", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:28:08.204377 [debug] [MainThread]: Command `dbt run` failed at 17:28:08.204324 after 0.79 seconds
[0m17:28:08.204587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1033c7f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1442f0250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1442f0730>]}
[0m17:28:08.204793 [debug] [MainThread]: Flushing usage events
[0m17:29:13.557667 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a40ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f04520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f04160>]}


============================== 17:29:13.560723 | 5401002c-0e40-41db-b518-e29b55be9789 ==============================
[0m17:29:13.560723 [info ] [MainThread]: Running with dbt=1.8.7
[0m17:29:13.561079 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/Users/ankurchopra/.dbt', 'log_path': '/Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:29:13.603503 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:29:13.603863 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:29:13.604050 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:29:14.325518 [error] [MainThread]: Encountered an error:
Runtime Error
  at path ['name']: 'Business Accounts' does not match '^[^\\d\\W]\\w*$'

Error encountered in /Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/dbt_project.yml
[0m17:29:14.327517 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 0.8089034, "process_user_time": 1.576854, "process_kernel_time": 2.308342, "process_mem_max_rss": "201211904", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:29:14.327838 [debug] [MainThread]: Command `dbt run` failed at 17:29:14.327777 after 0.81 seconds
[0m17:29:14.328044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a40ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13acbc610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13acbcfd0>]}
[0m17:29:14.328249 [debug] [MainThread]: Flushing usage events
[0m17:30:07.675178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102bcff10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10466e550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10466e130>]}


============================== 17:30:07.677959 | d4f32afb-9dfb-4d3c-8f1c-fcd807fdda1a ==============================
[0m17:30:07.677959 [info ] [MainThread]: Running with dbt=1.8.7
[0m17:30:07.678294 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/Users/ankurchopra/.dbt', 'log_path': '/Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m17:30:07.717963 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:30:07.718315 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:30:07.718502 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:30:08.385428 [error] [MainThread]: Encountered an error:
Runtime Error
  at path ['name']: 'Business Accounts' does not match '^[^\\d\\W]\\w*$'

Error encountered in /Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/dbt_project.yml
[0m17:30:08.387334 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 0.7509129, "process_user_time": 1.474799, "process_kernel_time": 2.315981, "process_mem_max_rss": "200327168", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:30:08.387651 [debug] [MainThread]: Command `dbt run` failed at 17:30:08.387597 after 0.75 seconds
[0m17:30:08.387878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102bcff10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11afb1610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11afb1220>]}
[0m17:30:08.388093 [debug] [MainThread]: Flushing usage events
[0m17:30:50.739403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10262ceb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1040cb4f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1040cb100>]}


============================== 17:30:50.742390 | 975621ea-cf54-4a49-93f6-a741db597788 ==============================
[0m17:30:50.742390 [info ] [MainThread]: Running with dbt=1.8.7
[0m17:30:50.742738 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/Users/ankurchopra/.dbt', 'log_path': '/Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m17:30:50.784690 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:30:50.785053 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:30:50.785248 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:30:51.562251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '975621ea-cf54-4a49-93f6-a741db597788', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1044ff1c0>]}
[0m17:30:51.590096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '975621ea-cf54-4a49-93f6-a741db597788', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1040969d0>]}
[0m17:30:51.590476 [info ] [MainThread]: Registered adapter: databricks=1.8.7
[0m17:30:51.606370 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m17:30:51.606999 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m17:30:51.607369 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '975621ea-cf54-4a49-93f6-a741db597788', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12df2cb50>]}
[0m17:30:52.545268 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.business_accounts.prepared_business_accounts' (models/business_accounts/prepared/prepared_business_accounts.sql) depends on a node named 'raw_transactions' which was not found
[0m17:30:52.548206 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.8469164, "process_user_time": 2.461559, "process_kernel_time": 2.32082, "process_mem_max_rss": "209108992", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:30:52.548565 [debug] [MainThread]: Command `dbt run` failed at 17:30:52.548507 after 1.85 seconds
[0m17:30:52.548824 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10262ceb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e62e970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e51e550>]}
[0m17:30:52.549071 [debug] [MainThread]: Flushing usage events
[0m17:33:06.396227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10673ff10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081df550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081df130>]}


============================== 17:33:06.399235 | 4f3b7cca-f24b-402a-bb8a-6bb61e51cf64 ==============================
[0m17:33:06.399235 [info ] [MainThread]: Running with dbt=1.8.7
[0m17:33:06.399584 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/ankurchopra/.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt run', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:33:06.441410 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:33:06.441759 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:33:06.441952 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:33:07.186413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4f3b7cca-f24b-402a-bb8a-6bb61e51cf64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081aac70>]}
[0m17:33:07.213792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4f3b7cca-f24b-402a-bb8a-6bb61e51cf64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081aa9d0>]}
[0m17:33:07.214156 [info ] [MainThread]: Registered adapter: databricks=1.8.7
[0m17:33:07.229247 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m17:33:07.229850 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m17:33:07.230200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '4f3b7cca-f24b-402a-bb8a-6bb61e51cf64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126029940>]}
[0m17:33:08.209845 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.business_accounts.prepared
- models.business_accounts.staging
- models.business_accounts.integrated
[0m17:33:08.216953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4f3b7cca-f24b-402a-bb8a-6bb61e51cf64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126944130>]}
[0m17:33:08.278929 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4f3b7cca-f24b-402a-bb8a-6bb61e51cf64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1268b9f70>]}
[0m17:33:08.279319 [info ] [MainThread]: Found 3 models, 2 sources, 603 macros
[0m17:33:08.279523 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4f3b7cca-f24b-402a-bb8a-6bb61e51cf64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12698e520>]}
[0m17:33:08.280354 [info ] [MainThread]: 
[0m17:33:08.280727 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4942555600, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(23550, 8635354944), compute-name=) - Creating connection
[0m17:33:08.280919 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:33:08.281091 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4942555600, session-id=None, name=master, idle-time=1.9073486328125e-06s, acquire-count=1, language=None, thread-identifier=(23550, 8635354944), compute-name=) - Acquired connection on thread (23550, 8635354944), using default compute resource
[0m17:33:08.284899 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(23550, 6140014592), compute-name=) - Creating connection
[0m17:33:08.285159 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m17:33:08.285342 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=None, name=list_hive_metastore, idle-time=9.5367431640625e-07s, acquire-count=1, language=None, thread-identifier=(23550, 6140014592), compute-name=) - Acquired connection on thread (23550, 6140014592), using default compute resource
[0m17:33:08.285531 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=None, name=list_hive_metastore, idle-time=0.0001971721649169922s, acquire-count=1, language=None, thread-identifier=(23550, 6140014592), compute-name=) - Checking idleness
[0m17:33:08.285695 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=None, name=list_hive_metastore, idle-time=0.0003619194030761719s, acquire-count=1, language=None, thread-identifier=(23550, 6140014592), compute-name=) - Retrieving connection
[0m17:33:08.285839 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m17:33:08.285986 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m17:33:08.286130 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:33:08.910615 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, name=list_hive_metastore, idle-time=1.8835067749023438e-05s, acquire-count=1, language=None, thread-identifier=(23550, 6140014592), compute-name=) - Connection created
[0m17:33:08.911670 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, command-id=Unknown) - Created cursor
[0m17:33:18.673299 [debug] [ThreadPool]: SQL status: OK in 10.390 seconds
[0m17:33:18.684385 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, command-id=01ef9962-10f7-1934-a519-1c504e7c07bc) - Closing cursor
[0m17:33:18.685059 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, name=list_hive_metastore, idle-time=5.245208740234375e-06s, acquire-count=0, language=None, thread-identifier=(23550, 6140014592), compute-name=) - Released connection
[0m17:33:18.687295 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, name=list_hive_metastore, idle-time=0.0022461414337158203s, acquire-count=0, language=None, thread-identifier=(23550, 6140014592), compute-name=) - Checking idleness
[0m17:33:18.687673 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m17:33:18.687971 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, name=list_hive_metastore_default, idle-time=0.0029299259185791016s, acquire-count=0, language=None, thread-identifier=(23550, 6140014592), compute-name=) - Reusing connection previously named list_hive_metastore
[0m17:33:18.688243 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, name=list_hive_metastore_default, idle-time=0.003216981887817383s, acquire-count=1, language=None, thread-identifier=(23550, 6140014592), compute-name=) - Acquired connection on thread (23550, 6140014592), using default compute resource
[0m17:33:18.688536 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, name=list_hive_metastore_default, idle-time=0.0035169124603271484s, acquire-count=1, language=None, thread-identifier=(23550, 6140014592), compute-name=) - Checking idleness
[0m17:33:18.688778 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, name=list_hive_metastore_default, idle-time=0.003760099411010742s, acquire-count=1, language=None, thread-identifier=(23550, 6140014592), compute-name=) - Retrieving connection
[0m17:33:18.688993 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:33:18.689209 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m17:33:18.689465 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, command-id=Unknown) - Created cursor
[0m17:33:19.261905 [debug] [ThreadPool]: SQL status: OK in 0.570 seconds
[0m17:33:19.266620 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, command-id=01ef9962-13b5-12ac-b127-9e1778f0e292) - Closing cursor
[0m17:33:19.279812 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, name=list_hive_metastore_default, idle-time=0.5947110652923584s, acquire-count=1, language=None, thread-identifier=(23550, 6140014592), compute-name=) - Checking idleness
[0m17:33:19.280244 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, name=list_hive_metastore_default, idle-time=0.5952131748199463s, acquire-count=1, language=None, thread-identifier=(23550, 6140014592), compute-name=) - Retrieving connection
[0m17:33:19.280537 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, name=list_hive_metastore_default, idle-time=0.5955159664154053s, acquire-count=1, language=None, thread-identifier=(23550, 6140014592), compute-name=) - Checking idleness
[0m17:33:19.280803 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, name=list_hive_metastore_default, idle-time=0.5957822799682617s, acquire-count=1, language=None, thread-identifier=(23550, 6140014592), compute-name=) - Retrieving connection
[0m17:33:19.281056 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:33:19.281287 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:33:19.281548 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m17:33:19.281832 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, command-id=Unknown) - Created cursor
[0m17:33:20.449013 [debug] [ThreadPool]: SQL status: OK in 1.170 seconds
[0m17:33:20.452391 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, command-id=01ef9962-140e-1800-8b5a-815d31b147d1) - Closing cursor
[0m17:33:20.461972 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, name=list_hive_metastore_default, idle-time=1.7769041061401367s, acquire-count=1, language=None, thread-identifier=(23550, 6140014592), compute-name=) - Checking idleness
[0m17:33:20.462408 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, name=list_hive_metastore_default, idle-time=1.7773730754852295s, acquire-count=1, language=None, thread-identifier=(23550, 6140014592), compute-name=) - Retrieving connection
[0m17:33:20.462676 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:33:20.462951 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m17:33:20.463251 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, command-id=Unknown) - Created cursor
[0m17:33:21.021615 [debug] [ThreadPool]: SQL status: OK in 0.560 seconds
[0m17:33:21.026763 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, command-id=01ef9962-14ba-121f-a71c-90fb8666f2ab) - Closing cursor
[0m17:33:21.027744 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, name=list_hive_metastore_default, idle-time=5.245208740234375e-06s, acquire-count=0, language=None, thread-identifier=(23550, 6140014592), compute-name=) - Released connection
[0m17:33:21.029816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4f3b7cca-f24b-402a-bb8a-6bb61e51cf64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125d5c280>]}
[0m17:33:21.030428 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4942555600, session-id=None, name=master, idle-time=12.749299049377441s, acquire-count=1, language=None, thread-identifier=(23550, 8635354944), compute-name=) - Checking idleness
[0m17:33:21.030793 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4942555600, session-id=None, name=master, idle-time=12.749669075012207s, acquire-count=1, language=None, thread-identifier=(23550, 8635354944), compute-name=) - Retrieving connection
[0m17:33:21.031140 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4942555600, session-id=None, name=master, idle-time=12.750026226043701s, acquire-count=1, language=None, thread-identifier=(23550, 8635354944), compute-name=) - Checking idleness
[0m17:33:21.031477 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4942555600, session-id=None, name=master, idle-time=12.75036096572876s, acquire-count=1, language=None, thread-identifier=(23550, 8635354944), compute-name=) - Retrieving connection
[0m17:33:21.031788 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:33:21.032093 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:33:21.032417 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4942555600, session-id=None, name=master, idle-time=2.1457672119140625e-06s, acquire-count=0, language=None, thread-identifier=(23550, 8635354944), compute-name=) - Released connection
[0m17:33:21.032961 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m17:33:21.033363 [info ] [MainThread]: 
[0m17:33:21.036987 [debug] [Thread-1  ]: Began running node model.business_accounts.staging_business_accounts
[0m17:33:21.037626 [info ] [Thread-1  ]: 1 of 3 START sql view model default.staging_business_accounts .................. [RUN]
[0m17:33:21.038229 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, name=list_hive_metastore_default, idle-time=0.010453939437866211s, acquire-count=0, language=None, thread-identifier=(23550, 6140014592), compute-name=) - Checking idleness
[0m17:33:21.038571 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.business_accounts.staging_business_accounts)
[0m17:33:21.038964 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, name=model.business_accounts.staging_business_accounts, idle-time=0.011205196380615234s, acquire-count=0, language=None, thread-identifier=(23550, 6140014592), compute-name=) - Reusing connection previously named list_hive_metastore_default
[0m17:33:21.039340 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, name=model.business_accounts.staging_business_accounts, idle-time=0.011578083038330078s, acquire-count=1, language=sql, thread-identifier=(23550, 6140014592), compute-name=) - Acquired connection on thread (23550, 6140014592), using default compute resource for model '`hive_metastore`.`default`.`staging_business_accounts`'
[0m17:33:21.039686 [debug] [Thread-1  ]: Began compiling node model.business_accounts.staging_business_accounts
[0m17:33:21.048567 [debug] [Thread-1  ]: Writing injected SQL for node "model.business_accounts.staging_business_accounts"
[0m17:33:21.049468 [debug] [Thread-1  ]: Began executing node model.business_accounts.staging_business_accounts
[0m17:33:21.069467 [debug] [Thread-1  ]: Writing runtime sql for node "model.business_accounts.staging_business_accounts"
[0m17:33:21.070446 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, name=model.business_accounts.staging_business_accounts, idle-time=0.04270792007446289s, acquire-count=1, language=sql, thread-identifier=(23550, 6140014592), compute-name=) - Checking idleness
[0m17:33:21.070715 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, name=model.business_accounts.staging_business_accounts, idle-time=0.04300117492675781s, acquire-count=1, language=sql, thread-identifier=(23550, 6140014592), compute-name=) - Retrieving connection
[0m17:33:21.070899 [debug] [Thread-1  ]: Using databricks connection "model.business_accounts.staging_business_accounts"
[0m17:33:21.071153 [debug] [Thread-1  ]: On model.business_accounts.staging_business_accounts: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.staging_business_accounts"} */
create or replace view `hive_metastore`.`default`.`staging_business_accounts`
  
  
  
  as
    

with staging_business_accounts as (
    select
        account_id,
        business_name as account_name,
        contact_email,
        to_date(registration_date, 'YYYY-MM-DD') as formatted_registration_date
    from `hive_metastore`.`raw`.`raw_business_accounts`
    where contact_email is not null
)

select *
from staging_business_accounts;

[0m17:33:21.071411 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, command-id=Unknown) - Created cursor
[0m17:33:23.002716 [debug] [Thread-1  ]: SQL status: OK in 1.930 seconds
[0m17:33:23.004914 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, command-id=01ef9962-1517-12b8-b84f-d2d912cc4bac) - Closing cursor
[0m17:33:23.025761 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, name=model.business_accounts.staging_business_accounts, idle-time=5.0067901611328125e-06s, acquire-count=0, language=sql, thread-identifier=(23550, 6140014592), compute-name=) - Released connection
[0m17:33:23.026369 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=4942972624, session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289, name=model.business_accounts.staging_business_accounts, idle-time=2.1457672119140625e-06s, acquire-count=0, language=sql, thread-identifier=(23550, 6140014592), compute-name=) - Released connection
[0m17:33:23.027797 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4f3b7cca-f24b-402a-bb8a-6bb61e51cf64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126aa8790>]}
[0m17:33:23.028478 [info ] [Thread-1  ]: 1 of 3 OK created sql view model default.staging_business_accounts ............. [[32mOK[0m in 1.99s]
[0m17:33:23.029024 [debug] [Thread-1  ]: Finished running node model.business_accounts.staging_business_accounts
[0m17:33:23.029842 [debug] [Thread-3  ]: Began running node model.business_accounts.integrated_business_accounts
[0m17:33:23.030398 [info ] [Thread-3  ]: 2 of 3 START sql view model default.integrated_business_accounts ............... [RUN]
[0m17:33:23.030926 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=4940665040, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0s, acquire-count=0, language=None, thread-identifier=(23550, 6179975168), compute-name=) - Creating connection
[0m17:33:23.031248 [debug] [Thread-3  ]: Acquiring new databricks connection 'model.business_accounts.integrated_business_accounts'
[0m17:33:23.031574 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=4940665040, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=2.1457672119140625e-06s, acquire-count=1, language=sql, thread-identifier=(23550, 6179975168), compute-name=) - Acquired connection on thread (23550, 6179975168), using default compute resource for model '`hive_metastore`.`default`.`integrated_business_accounts`'
[0m17:33:23.031877 [debug] [Thread-3  ]: Began compiling node model.business_accounts.integrated_business_accounts
[0m17:33:23.035975 [debug] [Thread-3  ]: Writing injected SQL for node "model.business_accounts.integrated_business_accounts"
[0m17:33:23.036858 [debug] [Thread-3  ]: Began executing node model.business_accounts.integrated_business_accounts
[0m17:33:23.040699 [debug] [Thread-3  ]: Writing runtime sql for node "model.business_accounts.integrated_business_accounts"
[0m17:33:23.041372 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=4940665040, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0.009812116622924805s, acquire-count=1, language=sql, thread-identifier=(23550, 6179975168), compute-name=) - Checking idleness
[0m17:33:23.041673 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=4940665040, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0.010128021240234375s, acquire-count=1, language=sql, thread-identifier=(23550, 6179975168), compute-name=) - Retrieving connection
[0m17:33:23.041945 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=4940665040, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0.01040792465209961s, acquire-count=1, language=sql, thread-identifier=(23550, 6179975168), compute-name=) - Checking idleness
[0m17:33:23.042210 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=4940665040, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0.010671854019165039s, acquire-count=1, language=sql, thread-identifier=(23550, 6179975168), compute-name=) - Retrieving connection
[0m17:33:23.042450 [debug] [Thread-3  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:33:23.042632 [debug] [Thread-3  ]: Using databricks connection "model.business_accounts.integrated_business_accounts"
[0m17:33:23.042950 [debug] [Thread-3  ]: On model.business_accounts.integrated_business_accounts: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.integrated_business_accounts"} */
create or replace view `hive_metastore`.`default`.`integrated_business_accounts`
  
  
  
  as
    

with deduplicated_accounts as (
    select distinct
        account_id,
        account_name,
        contact_email,
        formatted_registration_date
    from `hive_metastore`.`default`.`staging_business_accounts`
),
transactions_sum as (
    select
        account_id,
        sum(transaction_amount) as total_transactions_amount
    from `hive_metastore`.`raw`.`raw_transactions`
    group by account_id
),
integrated_business_accounts as (
    select
        a.account_id,
        a.account_name,
        a.contact_email,
        a.formatted_registration_date,
        coalesce(t.total_transactions_amount, 0) as total_transactions_amount
    from deduplicated_accounts a
    left join transactions_sum t on a.account_id = t.account_id
    order by a.formatted_registration_date desc
)

select *
from integrated_business_accounts;

[0m17:33:23.043248 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m17:33:23.374090 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=4940665040, session-id=01ef9962-1664-19f9-915f-4161839f1eef, name=model.business_accounts.integrated_business_accounts, idle-time=2.384185791015625e-05s, acquire-count=1, language=sql, thread-identifier=(23550, 6179975168), compute-name=) - Connection created
[0m17:33:23.375274 [debug] [Thread-3  ]: Databricks adapter: Cursor(session-id=01ef9962-1664-19f9-915f-4161839f1eef, command-id=Unknown) - Created cursor
[0m17:33:24.558227 [debug] [Thread-3  ]: SQL status: OK in 1.510 seconds
[0m17:33:24.560364 [debug] [Thread-3  ]: Databricks adapter: Cursor(session-id=01ef9962-1664-19f9-915f-4161839f1eef, command-id=01ef9962-1676-1958-b1ab-12320b2b7c19) - Closing cursor
[0m17:33:24.562129 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=4940665040, session-id=01ef9962-1664-19f9-915f-4161839f1eef, name=model.business_accounts.integrated_business_accounts, idle-time=6.9141387939453125e-06s, acquire-count=0, language=sql, thread-identifier=(23550, 6179975168), compute-name=) - Released connection
[0m17:33:24.562841 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=4940665040, session-id=01ef9962-1664-19f9-915f-4161839f1eef, name=model.business_accounts.integrated_business_accounts, idle-time=2.86102294921875e-06s, acquire-count=0, language=sql, thread-identifier=(23550, 6179975168), compute-name=) - Released connection
[0m17:33:24.563406 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4f3b7cca-f24b-402a-bb8a-6bb61e51cf64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126aa8790>]}
[0m17:33:24.564237 [info ] [Thread-3  ]: 2 of 3 OK created sql view model default.integrated_business_accounts .......... [[32mOK[0m in 1.53s]
[0m17:33:24.564994 [debug] [Thread-3  ]: Finished running node model.business_accounts.integrated_business_accounts
[0m17:33:24.566034 [debug] [Thread-2  ]: Began running node model.business_accounts.prepared_business_accounts
[0m17:33:24.566683 [info ] [Thread-2  ]: 3 of 3 START sql table model default.prepared_business_accounts ................ [RUN]
[0m17:33:24.567368 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=4944404336, session-id=None, name=model.business_accounts.prepared_business_accounts, idle-time=0s, acquire-count=0, language=None, thread-identifier=(23550, 6163148800), compute-name=) - Creating connection
[0m17:33:24.567804 [debug] [Thread-2  ]: Acquiring new databricks connection 'model.business_accounts.prepared_business_accounts'
[0m17:33:24.568232 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=4944404336, session-id=None, name=model.business_accounts.prepared_business_accounts, idle-time=3.0994415283203125e-06s, acquire-count=1, language=sql, thread-identifier=(23550, 6163148800), compute-name=) - Acquired connection on thread (23550, 6163148800), using default compute resource for model '`hive_metastore`.`default`.`prepared_business_accounts`'
[0m17:33:24.568633 [debug] [Thread-2  ]: Began compiling node model.business_accounts.prepared_business_accounts
[0m17:33:24.574667 [debug] [Thread-2  ]: Writing injected SQL for node "model.business_accounts.prepared_business_accounts"
[0m17:33:24.575630 [debug] [Thread-2  ]: Began executing node model.business_accounts.prepared_business_accounts
[0m17:33:24.586776 [debug] [Thread-2  ]: MATERIALIZING TABLE
[0m17:33:24.591631 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=4944404336, session-id=None, name=model.business_accounts.prepared_business_accounts, idle-time=0.023400306701660156s, acquire-count=1, language=sql, thread-identifier=(23550, 6163148800), compute-name=) - Checking idleness
[0m17:33:24.592014 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=4944404336, session-id=None, name=model.business_accounts.prepared_business_accounts, idle-time=0.023832082748413086s, acquire-count=1, language=sql, thread-identifier=(23550, 6163148800), compute-name=) - Retrieving connection
[0m17:33:24.592320 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=4944404336, session-id=None, name=model.business_accounts.prepared_business_accounts, idle-time=0.02414727210998535s, acquire-count=1, language=sql, thread-identifier=(23550, 6163148800), compute-name=) - Checking idleness
[0m17:33:24.592607 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=4944404336, session-id=None, name=model.business_accounts.prepared_business_accounts, idle-time=0.02443695068359375s, acquire-count=1, language=sql, thread-identifier=(23550, 6163148800), compute-name=) - Retrieving connection
[0m17:33:24.592870 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:33:24.593074 [debug] [Thread-2  ]: Using databricks connection "model.business_accounts.prepared_business_accounts"
[0m17:33:24.593346 [debug] [Thread-2  ]: On model.business_accounts.prepared_business_accounts: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.prepared_business_accounts"} */

      describe extended `hive_metastore`.`default`.`prepared_business_accounts`
  
[0m17:33:24.593604 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m17:33:24.968367 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=4944404336, session-id=01ef9962-1758-1597-bf99-1d8d04d47562, name=model.business_accounts.prepared_business_accounts, idle-time=6.9141387939453125e-06s, acquire-count=1, language=sql, thread-identifier=(23550, 6163148800), compute-name=) - Connection created
[0m17:33:24.969341 [debug] [Thread-2  ]: Databricks adapter: Cursor(session-id=01ef9962-1758-1597-bf99-1d8d04d47562, command-id=Unknown) - Created cursor
[0m17:33:25.665005 [debug] [Thread-2  ]: SQL status: OK in 1.070 seconds
[0m17:33:25.669596 [debug] [Thread-2  ]: Databricks adapter: Cursor(session-id=01ef9962-1758-1597-bf99-1d8d04d47562, command-id=01ef9962-1769-160c-9f2a-9bb37244914b) - Closing cursor
[0m17:33:25.699791 [debug] [Thread-2  ]: Writing runtime sql for node "model.business_accounts.prepared_business_accounts"
[0m17:33:25.700592 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=4944404336, session-id=01ef9962-1758-1597-bf99-1d8d04d47562, name=model.business_accounts.prepared_business_accounts, idle-time=0.7324919700622559s, acquire-count=1, language=sql, thread-identifier=(23550, 6163148800), compute-name=) - Checking idleness
[0m17:33:25.700923 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=4944404336, session-id=01ef9962-1758-1597-bf99-1d8d04d47562, name=model.business_accounts.prepared_business_accounts, idle-time=0.7328381538391113s, acquire-count=1, language=sql, thread-identifier=(23550, 6163148800), compute-name=) - Retrieving connection
[0m17:33:25.701172 [debug] [Thread-2  ]: Using databricks connection "model.business_accounts.prepared_business_accounts"
[0m17:33:25.701542 [debug] [Thread-2  ]: On model.business_accounts.prepared_business_accounts: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.prepared_business_accounts"} */

  
    
        create or replace table `hive_metastore`.`default`.`prepared_business_accounts`
      
      using delta
      
      
      
      
      
      
      
      as
      

with transaction_metrics as (
    select
        account_id,
        count(*) as total_number_of_transactions,
        
  sum(transaction_amount)
 as total_transaction_volume,
        min(transaction_date) as first_transaction_date,
        max(transaction_date) as last_transaction_date
    from `hive_metastore`.`raw`.`raw_transactions`
    group by account_id
),
account_data as (
    select
        a.account_id,
        a.account_name,
        a.contact_email,
        a.formatted_registration_date,
        coalesce(t.total_number_of_transactions, 0) as total_number_of_transactions,
        coalesce(t.total_transaction_volume, 0) as total_transaction_volume,
        t.first_transaction_date,
        t.last_transaction_date
    from `hive_metastore`.`default`.`integrated_business_accounts` a
    left join transaction_metrics t on a.account_id = t.account_id
)

select
    account_id,
    account_name,
    contact_email,
    formatted_registration_date,
    total_number_of_transactions,
    total_transaction_volume,
    first_transaction_date,
    last_transaction_date,
    
  CAST(current_timestamp() AS TIMESTAMP) as created_at,
  CAST(current_timestamp() AS TIMESTAMP) as updated_at,
  CAST(current_timestamp() AS TIMESTAMP) as processed_at

from account_data;
  
[0m17:33:25.701900 [debug] [Thread-2  ]: Databricks adapter: Cursor(session-id=01ef9962-1758-1597-bf99-1d8d04d47562, command-id=Unknown) - Created cursor
[0m17:33:32.812259 [debug] [Thread-2  ]: SQL status: OK in 7.110 seconds
[0m17:33:32.814836 [debug] [Thread-2  ]: Databricks adapter: Cursor(session-id=01ef9962-1758-1597-bf99-1d8d04d47562, command-id=01ef9962-17d9-115e-a1cc-72d6b0c11804) - Closing cursor
[0m17:33:33.063628 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=4944404336, session-id=01ef9962-1758-1597-bf99-1d8d04d47562, name=model.business_accounts.prepared_business_accounts, idle-time=5.9604644775390625e-06s, acquire-count=0, language=sql, thread-identifier=(23550, 6163148800), compute-name=) - Released connection
[0m17:33:33.064355 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=4944404336, session-id=01ef9962-1758-1597-bf99-1d8d04d47562, name=model.business_accounts.prepared_business_accounts, idle-time=1.9073486328125e-06s, acquire-count=0, language=sql, thread-identifier=(23550, 6163148800), compute-name=) - Released connection
[0m17:33:33.064830 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4f3b7cca-f24b-402a-bb8a-6bb61e51cf64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126aa8790>]}
[0m17:33:33.065540 [info ] [Thread-2  ]: 3 of 3 OK created sql table model default.prepared_business_accounts ........... [[32mOK[0m in 8.50s]
[0m17:33:33.066138 [debug] [Thread-2  ]: Finished running node model.business_accounts.prepared_business_accounts
[0m17:33:33.067475 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4942555600, session-id=None, name=master, idle-time=12.035032033920288s, acquire-count=0, language=None, thread-identifier=(23550, 8635354944), compute-name=) - Checking idleness
[0m17:33:33.067889 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4942555600, session-id=None, name=master, idle-time=12.035475969314575s, acquire-count=0, language=None, thread-identifier=(23550, 8635354944), compute-name=) - Reusing connection previously named master
[0m17:33:33.068190 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4942555600, session-id=None, name=master, idle-time=12.035780191421509s, acquire-count=1, language=None, thread-identifier=(23550, 8635354944), compute-name=) - Acquired connection on thread (23550, 8635354944), using default compute resource
[0m17:33:33.068520 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4942555600, session-id=None, name=master, idle-time=12.036121129989624s, acquire-count=1, language=None, thread-identifier=(23550, 8635354944), compute-name=) - Checking idleness
[0m17:33:33.068789 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4942555600, session-id=None, name=master, idle-time=12.03639030456543s, acquire-count=1, language=None, thread-identifier=(23550, 8635354944), compute-name=) - Retrieving connection
[0m17:33:33.069038 [debug] [MainThread]: On master: ROLLBACK
[0m17:33:33.069291 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:33:33.410009 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4942555600, session-id=01ef9962-1c61-17a5-b8d8-3c583de32f31, name=master, idle-time=8.106231689453125e-06s, acquire-count=1, language=None, thread-identifier=(23550, 8635354944), compute-name=) - Connection created
[0m17:33:33.410912 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:33:33.411521 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4942555600, session-id=01ef9962-1c61-17a5-b8d8-3c583de32f31, name=master, idle-time=0.0017049312591552734s, acquire-count=1, language=None, thread-identifier=(23550, 8635354944), compute-name=) - Checking idleness
[0m17:33:33.412046 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4942555600, session-id=01ef9962-1c61-17a5-b8d8-3c583de32f31, name=master, idle-time=0.002232074737548828s, acquire-count=1, language=None, thread-identifier=(23550, 8635354944), compute-name=) - Retrieving connection
[0m17:33:33.412517 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:33:33.412968 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:33:33.413469 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4942555600, session-id=01ef9962-1c61-17a5-b8d8-3c583de32f31, name=master, idle-time=3.0994415283203125e-06s, acquire-count=0, language=None, thread-identifier=(23550, 8635354944), compute-name=) - Released connection
[0m17:33:33.414198 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:33:33.414665 [debug] [MainThread]: On master: ROLLBACK
[0m17:33:33.415099 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:33:33.415443 [debug] [MainThread]: On master: Close
[0m17:33:33.415859 [debug] [MainThread]: Databricks adapter: Connection(session-id=01ef9962-1c61-17a5-b8d8-3c583de32f31) - Closing connection
[0m17:33:33.528628 [debug] [MainThread]: Connection 'model.business_accounts.staging_business_accounts' was properly closed.
[0m17:33:33.530321 [debug] [MainThread]: On model.business_accounts.staging_business_accounts: ROLLBACK
[0m17:33:33.530976 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:33:33.531452 [debug] [MainThread]: On model.business_accounts.staging_business_accounts: Close
[0m17:33:33.531960 [debug] [MainThread]: Databricks adapter: Connection(session-id=01ef9962-0db5-1bcd-b682-8dbedb56b289) - Closing connection
[0m17:33:33.630424 [debug] [MainThread]: Connection 'model.business_accounts.integrated_business_accounts' was properly closed.
[0m17:33:33.631153 [debug] [MainThread]: On model.business_accounts.integrated_business_accounts: ROLLBACK
[0m17:33:33.631677 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:33:33.632142 [debug] [MainThread]: On model.business_accounts.integrated_business_accounts: Close
[0m17:33:33.632629 [debug] [MainThread]: Databricks adapter: Connection(session-id=01ef9962-1664-19f9-915f-4161839f1eef) - Closing connection
[0m17:33:33.757976 [debug] [MainThread]: Connection 'model.business_accounts.prepared_business_accounts' was properly closed.
[0m17:33:33.759366 [debug] [MainThread]: On model.business_accounts.prepared_business_accounts: ROLLBACK
[0m17:33:33.760228 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:33:33.760832 [debug] [MainThread]: On model.business_accounts.prepared_business_accounts: Close
[0m17:33:33.761318 [debug] [MainThread]: Databricks adapter: Connection(session-id=01ef9962-1758-1597-bf99-1d8d04d47562) - Closing connection
[0m17:33:33.866996 [info ] [MainThread]: 
[0m17:33:33.867662 [info ] [MainThread]: Finished running 2 view models, 1 table model in 0 hours 0 minutes and 25.59 seconds (25.59s).
[0m17:33:33.869214 [debug] [MainThread]: Command end result
[0m17:33:33.912043 [info ] [MainThread]: 
[0m17:33:33.912451 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:33:33.912676 [info ] [MainThread]: 
[0m17:33:33.912896 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m17:33:33.915000 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 27.55868, "process_user_time": 3.053104, "process_kernel_time": 2.424104, "process_mem_max_rss": "214286336", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:33:33.915343 [debug] [MainThread]: Command `dbt run` succeeded at 17:33:33.915284 after 27.56 seconds
[0m17:33:33.915610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10673ff10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bd5130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125ffde50>]}
[0m17:33:33.915865 [debug] [MainThread]: Flushing usage events
[0m17:36:00.691730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078d7f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e7f550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e7f130>]}


============================== 17:36:00.694790 | 23a20325-08ea-4ac4-911f-79db0d38a711 ==============================
[0m17:36:00.694790 [info ] [MainThread]: Running with dbt=1.8.7
[0m17:36:00.695162 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/ankurchopra/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m17:36:00.736730 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:36:00.737092 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:36:00.737285 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:36:01.471969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '23a20325-08ea-4ac4-911f-79db0d38a711', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e4ac70>]}
[0m17:36:01.499528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '23a20325-08ea-4ac4-911f-79db0d38a711', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e4a9d0>]}
[0m17:36:01.499882 [info ] [MainThread]: Registered adapter: databricks=1.8.7
[0m17:36:01.514835 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m17:36:01.599457 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 5 files changed.
[0m17:36:01.599871 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/sources.yml
[0m17:36:01.600112 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/integrated/integrated_business_accounts.sql
[0m17:36:01.600330 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/prepared/prepared_business_accounts.sql
[0m17:36:01.600541 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/staging/staging_business_accounts.sql
[0m17:36:01.600742 [debug] [MainThread]: Partial parsing: updated file: business_accounts://macros/macros.sql
[0m17:36:01.704887 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.business_accounts.integrated_business_accounts' (models/business_accounts/integrated/integrated_business_accounts.sql) depends on a node named 'raw.raw_business_accounts' which was not found
[0m17:36:01.707952 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.0542469, "process_user_time": 1.742815, "process_kernel_time": 2.316883, "process_mem_max_rss": "208535552", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:36:01.708295 [debug] [MainThread]: Command `dbt run` failed at 17:36:01.708238 after 1.05 seconds
[0m17:36:01.708548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078d7f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x135ca6640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x135c655e0>]}
[0m17:36:01.708785 [debug] [MainThread]: Flushing usage events
[0m17:42:17.746371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110918ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112ebf4f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112ebf100>]}


============================== 17:42:17.749370 | d0efbe28-47ca-4de2-903f-a9fe850b5cff ==============================
[0m17:42:17.749370 [info ] [MainThread]: Running with dbt=1.8.7
[0m17:42:17.749695 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/logs', 'profiles_dir': '/Users/ankurchopra/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m17:42:17.791398 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:42:17.791753 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:42:17.791941 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:42:18.556889 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd0efbe28-47ca-4de2-903f-a9fe850b5cff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113bfea00>]}
[0m17:42:18.586013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd0efbe28-47ca-4de2-903f-a9fe850b5cff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15b7af040>]}
[0m17:42:18.586529 [info ] [MainThread]: Registered adapter: databricks=1.8.7
[0m17:42:18.602324 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m17:42:18.686576 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 5 files changed.
[0m17:42:18.686984 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/sources.yml
[0m17:42:18.687268 [debug] [MainThread]: Partial parsing: updated file: business_accounts://macros/macros.sql
[0m17:42:18.687481 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/integrated/integrated_business_accounts.sql
[0m17:42:18.687694 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/staging/staging_business_accounts.sql
[0m17:42:18.687880 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/prepared/prepared_business_accounts.sql
[0m17:42:18.790686 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.business_accounts.prepared_business_accounts' (models/business_accounts/prepared/prepared_business_accounts.sql) depends on a node named 'raw.raw_transactions' which was not found
[0m17:42:18.793551 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.0841248, "process_user_time": 1.766212, "process_kernel_time": 2.312169, "process_mem_max_rss": "211386368", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:42:18.793895 [debug] [MainThread]: Command `dbt run` failed at 17:42:18.793840 after 1.08 seconds
[0m17:42:18.794134 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110918ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15c3285e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15c303f70>]}
[0m17:42:18.794361 [debug] [MainThread]: Flushing usage events
[0m17:44:16.556946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10666bf10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10810a550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10810a130>]}


============================== 17:44:16.560080 | 944dff28-4aff-404f-8c70-79dc9f7c522f ==============================
[0m17:44:16.560080 [info ] [MainThread]: Running with dbt=1.8.7
[0m17:44:16.560446 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/logs', 'profiles_dir': '/Users/ankurchopra/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m17:44:16.602787 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:44:16.603151 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:44:16.603346 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:44:17.346253 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '944dff28-4aff-404f-8c70-79dc9f7c522f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1080d6c70>]}
[0m17:44:17.374113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '944dff28-4aff-404f-8c70-79dc9f7c522f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1080d69d0>]}
[0m17:44:17.374514 [info ] [MainThread]: Registered adapter: databricks=1.8.7
[0m17:44:17.389106 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m17:44:17.471961 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 4 files changed.
[0m17:44:17.472356 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/integrated/integrated_business_accounts.sql
[0m17:44:17.472576 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/staging/staging_business_accounts.sql
[0m17:44:17.472825 [debug] [MainThread]: Partial parsing: updated file: business_accounts://macros/macros.sql
[0m17:44:17.473010 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/prepared/prepared_business_accounts.sql
[0m17:44:17.600105 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.business_accounts.prepared
- models.business_accounts.staging
- models.business_accounts.integrated
[0m17:44:17.608224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '944dff28-4aff-404f-8c70-79dc9f7c522f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x135ed8130>]}
[0m17:44:17.717697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '944dff28-4aff-404f-8c70-79dc9f7c522f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10699b940>]}
[0m17:44:17.718061 [info ] [MainThread]: Found 3 models, 2 sources, 603 macros
[0m17:44:17.718258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '944dff28-4aff-404f-8c70-79dc9f7c522f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106dbe970>]}
[0m17:44:17.719058 [info ] [MainThread]: 
[0m17:44:17.719413 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4434625008, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(23700, 8635354944), compute-name=) - Creating connection
[0m17:44:17.719593 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:44:17.719773 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4434625008, session-id=None, name=master, idle-time=2.1457672119140625e-06s, acquire-count=1, language=None, thread-identifier=(23700, 8635354944), compute-name=) - Acquired connection on thread (23700, 8635354944), using default compute resource
[0m17:44:17.723685 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(23700, 6140882944), compute-name=) - Creating connection
[0m17:44:17.723977 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m17:44:17.724170 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=None, name=list_hive_metastore, idle-time=1.1920928955078125e-06s, acquire-count=1, language=None, thread-identifier=(23700, 6140882944), compute-name=) - Acquired connection on thread (23700, 6140882944), using default compute resource
[0m17:44:17.724361 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=None, name=list_hive_metastore, idle-time=0.0002009868621826172s, acquire-count=1, language=None, thread-identifier=(23700, 6140882944), compute-name=) - Checking idleness
[0m17:44:17.724538 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=None, name=list_hive_metastore, idle-time=0.0003809928894042969s, acquire-count=1, language=None, thread-identifier=(23700, 6140882944), compute-name=) - Retrieving connection
[0m17:44:17.724683 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m17:44:17.724842 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m17:44:17.724993 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:44:18.384340 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, name=list_hive_metastore, idle-time=9.059906005859375e-06s, acquire-count=1, language=None, thread-identifier=(23700, 6140882944), compute-name=) - Connection created
[0m17:44:18.385319 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, command-id=Unknown) - Created cursor
[0m17:44:28.010701 [debug] [ThreadPool]: SQL status: OK in 10.290 seconds
[0m17:44:28.012789 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, command-id=01ef9963-9ff4-1339-86ff-a13d1f6b11aa) - Closing cursor
[0m17:44:28.013119 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, name=list_hive_metastore, idle-time=2.1457672119140625e-06s, acquire-count=0, language=None, thread-identifier=(23700, 6140882944), compute-name=) - Released connection
[0m17:44:28.013911 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, name=list_hive_metastore, idle-time=0.0007710456848144531s, acquire-count=0, language=None, thread-identifier=(23700, 6140882944), compute-name=) - Checking idleness
[0m17:44:28.014168 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m17:44:28.014372 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, name=list_hive_metastore_default, idle-time=0.0012631416320800781s, acquire-count=0, language=None, thread-identifier=(23700, 6140882944), compute-name=) - Reusing connection previously named list_hive_metastore
[0m17:44:28.014563 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, name=list_hive_metastore_default, idle-time=0.001455068588256836s, acquire-count=1, language=None, thread-identifier=(23700, 6140882944), compute-name=) - Acquired connection on thread (23700, 6140882944), using default compute resource
[0m17:44:28.014851 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, name=list_hive_metastore_default, idle-time=0.0016598701477050781s, acquire-count=1, language=None, thread-identifier=(23700, 6140882944), compute-name=) - Checking idleness
[0m17:44:28.015040 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, name=list_hive_metastore_default, idle-time=0.0019369125366210938s, acquire-count=1, language=None, thread-identifier=(23700, 6140882944), compute-name=) - Retrieving connection
[0m17:44:28.015201 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:44:28.015358 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m17:44:28.015530 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, command-id=Unknown) - Created cursor
[0m17:44:28.419029 [debug] [ThreadPool]: SQL status: OK in 0.400 seconds
[0m17:44:28.423750 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, command-id=01ef9963-a2a3-1fae-b44e-3d344faf452b) - Closing cursor
[0m17:44:28.435798 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, name=list_hive_metastore_default, idle-time=0.4226391315460205s, acquire-count=1, language=None, thread-identifier=(23700, 6140882944), compute-name=) - Checking idleness
[0m17:44:28.436193 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, name=list_hive_metastore_default, idle-time=0.423065185546875s, acquire-count=1, language=None, thread-identifier=(23700, 6140882944), compute-name=) - Retrieving connection
[0m17:44:28.436479 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, name=list_hive_metastore_default, idle-time=0.42336010932922363s, acquire-count=1, language=None, thread-identifier=(23700, 6140882944), compute-name=) - Checking idleness
[0m17:44:28.436747 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, name=list_hive_metastore_default, idle-time=0.4236280918121338s, acquire-count=1, language=None, thread-identifier=(23700, 6140882944), compute-name=) - Retrieving connection
[0m17:44:28.436996 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:44:28.437219 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:44:28.437471 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m17:44:28.437746 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, command-id=Unknown) - Created cursor
[0m17:44:29.548043 [debug] [ThreadPool]: SQL status: OK in 1.110 seconds
[0m17:44:29.551839 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, command-id=01ef9963-a2e4-1fe0-8c14-0baae5097eed) - Closing cursor
[0m17:44:29.559991 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, name=list_hive_metastore_default, idle-time=1.546807050704956s, acquire-count=1, language=None, thread-identifier=(23700, 6140882944), compute-name=) - Checking idleness
[0m17:44:29.560476 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, name=list_hive_metastore_default, idle-time=1.5473289489746094s, acquire-count=1, language=None, thread-identifier=(23700, 6140882944), compute-name=) - Retrieving connection
[0m17:44:29.560826 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:44:29.561160 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m17:44:29.561492 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, command-id=Unknown) - Created cursor
[0m17:44:30.115102 [debug] [ThreadPool]: SQL status: OK in 0.550 seconds
[0m17:44:30.119412 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, command-id=01ef9963-a38f-1c41-a48e-5303ae3e2304) - Closing cursor
[0m17:44:30.120444 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, name=list_hive_metastore_default, idle-time=4.76837158203125e-06s, acquire-count=0, language=None, thread-identifier=(23700, 6140882944), compute-name=) - Released connection
[0m17:44:30.122468 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '944dff28-4aff-404f-8c70-79dc9f7c522f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108530250>]}
[0m17:44:30.123135 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4434625008, session-id=None, name=master, idle-time=12.403309106826782s, acquire-count=1, language=None, thread-identifier=(23700, 8635354944), compute-name=) - Checking idleness
[0m17:44:30.123551 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4434625008, session-id=None, name=master, idle-time=12.403743982315063s, acquire-count=1, language=None, thread-identifier=(23700, 8635354944), compute-name=) - Retrieving connection
[0m17:44:30.123927 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4434625008, session-id=None, name=master, idle-time=12.404128074645996s, acquire-count=1, language=None, thread-identifier=(23700, 8635354944), compute-name=) - Checking idleness
[0m17:44:30.124303 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4434625008, session-id=None, name=master, idle-time=12.404508113861084s, acquire-count=1, language=None, thread-identifier=(23700, 8635354944), compute-name=) - Retrieving connection
[0m17:44:30.124631 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:44:30.124943 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:44:30.125286 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4434625008, session-id=None, name=master, idle-time=2.1457672119140625e-06s, acquire-count=0, language=None, thread-identifier=(23700, 8635354944), compute-name=) - Released connection
[0m17:44:30.125816 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m17:44:30.126203 [info ] [MainThread]: 
[0m17:44:30.130013 [debug] [Thread-1  ]: Began running node model.business_accounts.staging_business_accounts
[0m17:44:30.130657 [info ] [Thread-1  ]: 1 of 3 START sql view model default.staging_business_accounts .................. [RUN]
[0m17:44:30.131235 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, name=list_hive_metastore_default, idle-time=0.010774850845336914s, acquire-count=0, language=None, thread-identifier=(23700, 6140882944), compute-name=) - Checking idleness
[0m17:44:30.131562 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.business_accounts.staging_business_accounts)
[0m17:44:30.131933 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, name=model.business_accounts.staging_business_accounts, idle-time=0.011487960815429688s, acquire-count=0, language=None, thread-identifier=(23700, 6140882944), compute-name=) - Reusing connection previously named list_hive_metastore_default
[0m17:44:30.132307 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, name=model.business_accounts.staging_business_accounts, idle-time=0.011856794357299805s, acquire-count=1, language=sql, thread-identifier=(23700, 6140882944), compute-name=) - Acquired connection on thread (23700, 6140882944), using default compute resource for model '`hive_metastore`.`default`.`staging_business_accounts`'
[0m17:44:30.132664 [debug] [Thread-1  ]: Began compiling node model.business_accounts.staging_business_accounts
[0m17:44:30.143025 [debug] [Thread-1  ]: Writing injected SQL for node "model.business_accounts.staging_business_accounts"
[0m17:44:30.144352 [debug] [Thread-1  ]: Began executing node model.business_accounts.staging_business_accounts
[0m17:44:30.164116 [debug] [Thread-1  ]: Writing runtime sql for node "model.business_accounts.staging_business_accounts"
[0m17:44:30.164775 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, name=model.business_accounts.staging_business_accounts, idle-time=0.044348716735839844s, acquire-count=1, language=sql, thread-identifier=(23700, 6140882944), compute-name=) - Checking idleness
[0m17:44:30.165051 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, name=model.business_accounts.staging_business_accounts, idle-time=0.04464364051818848s, acquire-count=1, language=sql, thread-identifier=(23700, 6140882944), compute-name=) - Retrieving connection
[0m17:44:30.165238 [debug] [Thread-1  ]: Using databricks connection "model.business_accounts.staging_business_accounts"
[0m17:44:30.165489 [debug] [Thread-1  ]: On model.business_accounts.staging_business_accounts: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.staging_business_accounts"} */
create or replace view `hive_metastore`.`default`.`staging_business_accounts`
  
  
  
  as
    

with staging_business_accounts as (
    select
        account_id,
        business_name as account_name,
        contact_email,
        to_date(registration_date, 'YYYY-MM-DD') as registration_date
    from `hive_metastore`.`raw`.`raw_business_accounts`
    where contact_email is not null
)

select *
from staging_business_accounts;

[0m17:44:30.165765 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, command-id=Unknown) - Created cursor
[0m17:44:32.107713 [debug] [Thread-1  ]: SQL status: OK in 1.940 seconds
[0m17:44:32.109939 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, command-id=01ef9963-a3e5-19df-9e12-ab546766d9cc) - Closing cursor
[0m17:44:32.129572 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, name=model.business_accounts.staging_business_accounts, idle-time=4.0531158447265625e-06s, acquire-count=0, language=sql, thread-identifier=(23700, 6140882944), compute-name=) - Released connection
[0m17:44:32.130130 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5197778320, session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2, name=model.business_accounts.staging_business_accounts, idle-time=2.1457672119140625e-06s, acquire-count=0, language=sql, thread-identifier=(23700, 6140882944), compute-name=) - Released connection
[0m17:44:32.131359 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '944dff28-4aff-404f-8c70-79dc9f7c522f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x134aca100>]}
[0m17:44:32.131996 [info ] [Thread-1  ]: 1 of 3 OK created sql view model default.staging_business_accounts ............. [[32mOK[0m in 2.00s]
[0m17:44:32.132549 [debug] [Thread-1  ]: Finished running node model.business_accounts.staging_business_accounts
[0m17:44:32.133266 [debug] [Thread-3  ]: Began running node model.business_accounts.integrated_business_accounts
[0m17:44:32.133908 [info ] [Thread-3  ]: 2 of 3 START sql view model default.integrated_business_accounts ............... [RUN]
[0m17:44:32.134478 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5198981152, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0s, acquire-count=0, language=None, thread-identifier=(23700, 6179696640), compute-name=) - Creating connection
[0m17:44:32.134824 [debug] [Thread-3  ]: Acquiring new databricks connection 'model.business_accounts.integrated_business_accounts'
[0m17:44:32.135165 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5198981152, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=1.9073486328125e-06s, acquire-count=1, language=sql, thread-identifier=(23700, 6179696640), compute-name=) - Acquired connection on thread (23700, 6179696640), using default compute resource for model '`hive_metastore`.`default`.`integrated_business_accounts`'
[0m17:44:32.135477 [debug] [Thread-3  ]: Began compiling node model.business_accounts.integrated_business_accounts
[0m17:44:32.139091 [debug] [Thread-3  ]: Writing injected SQL for node "model.business_accounts.integrated_business_accounts"
[0m17:44:32.139687 [debug] [Thread-3  ]: Began executing node model.business_accounts.integrated_business_accounts
[0m17:44:32.142286 [debug] [Thread-3  ]: Writing runtime sql for node "model.business_accounts.integrated_business_accounts"
[0m17:44:32.142797 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5198981152, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0.007648944854736328s, acquire-count=1, language=sql, thread-identifier=(23700, 6179696640), compute-name=) - Checking idleness
[0m17:44:32.143098 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5198981152, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0.007957935333251953s, acquire-count=1, language=sql, thread-identifier=(23700, 6179696640), compute-name=) - Retrieving connection
[0m17:44:32.143394 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5198981152, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0.008256912231445312s, acquire-count=1, language=sql, thread-identifier=(23700, 6179696640), compute-name=) - Checking idleness
[0m17:44:32.143672 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5198981152, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0.008537054061889648s, acquire-count=1, language=sql, thread-identifier=(23700, 6179696640), compute-name=) - Retrieving connection
[0m17:44:32.143938 [debug] [Thread-3  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:44:32.144143 [debug] [Thread-3  ]: Using databricks connection "model.business_accounts.integrated_business_accounts"
[0m17:44:32.144485 [debug] [Thread-3  ]: On model.business_accounts.integrated_business_accounts: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.integrated_business_accounts"} */
create or replace view `hive_metastore`.`default`.`integrated_business_accounts`
  
  
  
  as
    {
    config(
        materialized='view',
        unique_key='account_id'
    )
}

with deduplicated_accounts as (
    select
        account_id,
        max(account_name) as account_name,
        max(contact_email) as contact_email,
        max(registration_date) as registration_date
    from `hive_metastore`.`default`.`staging_business_accounts`
    group by account_id
),
transaction_totals as (
    select
        account_id,
        sum(transaction_amount) as total_transactions_amount
    from `hive_metastore`.`raw`.`raw_transactions`
    group by account_id
),
enriched_accounts as (
    select
        a.account_id,
        a.account_name,
        a.contact_email,
        a.registration_date,
        coalesce(t.total_transactions_amount, 0) as total_transactions_amount
    from deduplicated_accounts a
    left join transaction_totals t
        on a.account_id = t.account_id
)

select
    account_id,
    account_name,
    contact_email,
    registration_date,
    total_transactions_amount
from enriched_accounts
order by registration_date desc;

[0m17:44:32.144791 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m17:44:32.493474 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5198981152, session-id=01ef9963-a536-1eb9-9ebe-e1561d73dbe8, name=model.business_accounts.integrated_business_accounts, idle-time=2.7179718017578125e-05s, acquire-count=1, language=sql, thread-identifier=(23700, 6179696640), compute-name=) - Connection created
[0m17:44:32.494551 [debug] [Thread-3  ]: Databricks adapter: Cursor(session-id=01ef9963-a536-1eb9-9ebe-e1561d73dbe8, command-id=Unknown) - Created cursor
[0m17:44:32.781653 [debug] [Thread-3  ]: Databricks adapter: Cursor(session-id=01ef9963-a536-1eb9-9ebe-e1561d73dbe8, command-id=Unknown) - Closing cursor
[0m17:44:32.784149 [debug] [Thread-3  ]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.integrated_business_accounts"} */
create or replace view `hive_metastore`.`default`.`integrated_business_accounts`
  
  
  
  as
    {
    config(
        materialized='view',
        unique_key='account_id'
    )
}

with deduplicated_accounts as (
    select
        account_id,
        max(account_name) as account_name,
        max(contact_email) as contact_email,
        max(registration_date) as registration_date
    from `hive_metastore`.`default`.`staging_business_accounts`
    group by account_id
),
transaction_totals as (
    select
        account_id,
        sum(transaction_amount) as total_transactions_amount
    from `hive_metastore`.`raw`.`raw_transactions`
    group by account_id
),
enriched_accounts as (
    select
        a.account_id,
        a.account_name,
        a.contact_email,
        a.registration_date,
        coalesce(t.total_transactions_amount, 0) as total_transactions_amount
    from deduplicated_accounts a
    left join transaction_totals t
        on a.account_id = t.account_id
)

select
    account_id,
    account_name,
    contact_email,
    registration_date,
    total_transactions_amount
from enriched_accounts
order by registration_date desc;

: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '{'. SQLSTATE: 42601 (line 7, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.integrated_business_accounts"} */
create or replace view `hive_metastore`.`default`.`integrated_business_accounts`
  
  
  
  as
    {
----^^^
    config(
        materialized='view',
        unique_key='account_id'
    )
}

with deduplicated_accounts as (
    select
        account_id,
        max(account_name) as account_name,
        max(contact_email) as contact_email,
        max(registration_date) as registration_date
    from `hive_metastore`.`default`.`staging_business_accounts`
    group by account_id
),
transaction_totals as (
    select
        account_id,
        sum(transaction_amount) as total_transactions_amount
    from `hive_metastore`.`raw`.`raw_transactions`
    group by account_id
),
enriched_accounts as (
    select
        a.account_id,
        a.account_name,
        a.contact_email,
        a.registration_date,
        coalesce(t.total_transactions_amount, 0) as total_transactions_amount
    from deduplicated_accounts a
    left join transaction_totals t
        on a.account_id = t.account_id
)

select
    account_id,
    account_name,
    contact_email,
    registration_date,
    total_transactions_amount
from enriched_accounts
order by registration_date desc

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '{'. SQLSTATE: 42601 (line 7, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.integrated_business_accounts"} */
create or replace view `hive_metastore`.`default`.`integrated_business_accounts`
  
  
  
  as
    {
----^^^
    config(
        materialized='view',
        unique_key='account_id'
    )
}

with deduplicated_accounts as (
    select
        account_id,
        max(account_name) as account_name,
        max(contact_email) as contact_email,
        max(registration_date) as registration_date
    from `hive_metastore`.`default`.`staging_business_accounts`
    group by account_id
),
transaction_totals as (
    select
        account_id,
        sum(transaction_amount) as total_transactions_amount
    from `hive_metastore`.`raw`.`raw_transactions`
    group by account_id
),
enriched_accounts as (
    select
        a.account_id,
        a.account_name,
        a.contact_email,
        a.registration_date,
        coalesce(t.total_transactions_amount, 0) as total_transactions_amount
    from deduplicated_accounts a
    left join transaction_totals t
        on a.account_id = t.account_id
)

select
    account_id,
    account_name,
    contact_email,
    registration_date,
    total_transactions_amount
from enriched_accounts
order by registration_date desc

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:786)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:624)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:469)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:704)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:469)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:74)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:174)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:446)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:432)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:482)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '{'. SQLSTATE: 42601 (line 7, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.integrated_business_accounts"} */
create or replace view `hive_metastore`.`default`.`integrated_business_accounts`
  
  
  
  as
    {
----^^^
    config(
        materialized='view',
        unique_key='account_id'
    )
}

with deduplicated_accounts as (
    select
        account_id,
        max(account_name) as account_name,
        max(contact_email) as contact_email,
        max(registration_date) as registration_date
    from `hive_metastore`.`default`.`staging_business_accounts`
    group by account_id
),
transaction_totals as (
    select
        account_id,
        sum(transaction_amount) as total_transactions_amount
    from `hive_metastore`.`raw`.`raw_transactions`
    group by account_id
),
enriched_accounts as (
    select
        a.account_id,
        a.account_name,
        a.contact_email,
        a.registration_date,
        coalesce(t.total_transactions_amount, 0) as total_transactions_amount
    from deduplicated_accounts a
    left join transaction_totals t
        on a.account_id = t.account_id
)

select
    account_id,
    account_name,
    contact_email,
    registration_date,
    total_transactions_amount
from enriched_accounts
order by registration_date desc

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:308)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:114)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:137)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:106)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:80)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:101)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:77)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:383)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:453)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:382)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:143)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:382)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:668)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:665)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:485)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:602)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:598)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:113)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:112)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:89)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:154)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)
	... 3 more
, operation-id=01ef9963-a549-1d2c-a585-e861f0812247
[0m17:44:32.786297 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5198981152, session-id=01ef9963-a536-1eb9-9ebe-e1561d73dbe8, name=model.business_accounts.integrated_business_accounts, idle-time=7.867813110351562e-06s, acquire-count=0, language=sql, thread-identifier=(23700, 6179696640), compute-name=) - Released connection
[0m17:44:32.797254 [debug] [Thread-3  ]: Database Error in model integrated_business_accounts (models/business_accounts/integrated/integrated_business_accounts.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '{'. SQLSTATE: 42601 (line 7, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.integrated_business_accounts"} */
  create or replace view `hive_metastore`.`default`.`integrated_business_accounts`
    
    
    
    as
      {
  ----^^^
      config(
          materialized='view',
          unique_key='account_id'
      )
  }
  
  with deduplicated_accounts as (
      select
          account_id,
          max(account_name) as account_name,
          max(contact_email) as contact_email,
          max(registration_date) as registration_date
      from `hive_metastore`.`default`.`staging_business_accounts`
      group by account_id
  ),
  transaction_totals as (
      select
          account_id,
          sum(transaction_amount) as total_transactions_amount
      from `hive_metastore`.`raw`.`raw_transactions`
      group by account_id
  ),
  enriched_accounts as (
      select
          a.account_id,
          a.account_name,
          a.contact_email,
          a.registration_date,
          coalesce(t.total_transactions_amount, 0) as total_transactions_amount
      from deduplicated_accounts a
      left join transaction_totals t
          on a.account_id = t.account_id
  )
  
  select
      account_id,
      account_name,
      contact_email,
      registration_date,
      total_transactions_amount
  from enriched_accounts
  order by registration_date desc
  
  compiled code at target/run/business_accounts/models/business_accounts/integrated/integrated_business_accounts.sql
[0m17:44:32.797960 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5198981152, session-id=01ef9963-a536-1eb9-9ebe-e1561d73dbe8, name=model.business_accounts.integrated_business_accounts, idle-time=3.0994415283203125e-06s, acquire-count=0, language=sql, thread-identifier=(23700, 6179696640), compute-name=) - Released connection
[0m17:44:32.798507 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '944dff28-4aff-404f-8c70-79dc9f7c522f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x135840f40>]}
[0m17:44:32.799380 [error] [Thread-3  ]: 2 of 3 ERROR creating sql view model default.integrated_business_accounts ...... [[31mERROR[0m in 0.66s]
[0m17:44:32.800175 [debug] [Thread-3  ]: Finished running node model.business_accounts.integrated_business_accounts
[0m17:44:32.801100 [debug] [Thread-2  ]: Began running node model.business_accounts.prepared_business_accounts
[0m17:44:32.801596 [info ] [Thread-2  ]: 3 of 3 SKIP relation default.prepared_business_accounts ........................ [[33mSKIP[0m]
[0m17:44:32.802010 [debug] [Thread-2  ]: Finished running node model.business_accounts.prepared_business_accounts
[0m17:44:32.803169 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4434625008, session-id=None, name=master, idle-time=2.67785906791687s, acquire-count=0, language=None, thread-identifier=(23700, 8635354944), compute-name=) - Checking idleness
[0m17:44:32.803594 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4434625008, session-id=None, name=master, idle-time=2.6783082485198975s, acquire-count=0, language=None, thread-identifier=(23700, 8635354944), compute-name=) - Reusing connection previously named master
[0m17:44:32.803928 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4434625008, session-id=None, name=master, idle-time=2.6786420345306396s, acquire-count=1, language=None, thread-identifier=(23700, 8635354944), compute-name=) - Acquired connection on thread (23700, 8635354944), using default compute resource
[0m17:44:32.804279 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4434625008, session-id=None, name=master, idle-time=2.6790030002593994s, acquire-count=1, language=None, thread-identifier=(23700, 8635354944), compute-name=) - Checking idleness
[0m17:44:32.804583 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4434625008, session-id=None, name=master, idle-time=2.6793081760406494s, acquire-count=1, language=None, thread-identifier=(23700, 8635354944), compute-name=) - Retrieving connection
[0m17:44:32.804867 [debug] [MainThread]: On master: ROLLBACK
[0m17:44:32.805147 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:44:33.134565 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4434625008, session-id=01ef9963-a598-1cba-8da3-a4f6d39b3d63, name=master, idle-time=2.8848648071289062e-05s, acquire-count=1, language=None, thread-identifier=(23700, 8635354944), compute-name=) - Connection created
[0m17:44:33.136241 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:44:33.137272 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4434625008, session-id=01ef9963-a598-1cba-8da3-a4f6d39b3d63, name=master, idle-time=0.003114938735961914s, acquire-count=1, language=None, thread-identifier=(23700, 8635354944), compute-name=) - Checking idleness
[0m17:44:33.137913 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4434625008, session-id=01ef9963-a598-1cba-8da3-a4f6d39b3d63, name=master, idle-time=0.0038568973541259766s, acquire-count=1, language=None, thread-identifier=(23700, 8635354944), compute-name=) - Retrieving connection
[0m17:44:33.138414 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:44:33.138877 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:44:33.139425 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=4434625008, session-id=01ef9963-a598-1cba-8da3-a4f6d39b3d63, name=master, idle-time=3.0994415283203125e-06s, acquire-count=0, language=None, thread-identifier=(23700, 8635354944), compute-name=) - Released connection
[0m17:44:33.140180 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:44:33.140668 [debug] [MainThread]: On master: ROLLBACK
[0m17:44:33.141127 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:44:33.141552 [debug] [MainThread]: On master: Close
[0m17:44:33.142040 [debug] [MainThread]: Databricks adapter: Connection(session-id=01ef9963-a598-1cba-8da3-a4f6d39b3d63) - Closing connection
[0m17:44:33.247283 [debug] [MainThread]: Connection 'model.business_accounts.staging_business_accounts' was properly closed.
[0m17:44:33.248467 [debug] [MainThread]: On model.business_accounts.staging_business_accounts: ROLLBACK
[0m17:44:33.249084 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:44:33.249805 [debug] [MainThread]: On model.business_accounts.staging_business_accounts: Close
[0m17:44:33.250336 [debug] [MainThread]: Databricks adapter: Connection(session-id=01ef9963-9cb5-1ee3-908f-e4757f90dab2) - Closing connection
[0m17:44:33.353419 [debug] [MainThread]: Connection 'model.business_accounts.integrated_business_accounts' was properly closed.
[0m17:44:33.354762 [debug] [MainThread]: On model.business_accounts.integrated_business_accounts: ROLLBACK
[0m17:44:33.355431 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:44:33.355916 [debug] [MainThread]: On model.business_accounts.integrated_business_accounts: Close
[0m17:44:33.356404 [debug] [MainThread]: Databricks adapter: Connection(session-id=01ef9963-a536-1eb9-9ebe-e1561d73dbe8) - Closing connection
[0m17:44:33.471542 [info ] [MainThread]: 
[0m17:44:33.472328 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 15.75 seconds (15.75s).
[0m17:44:33.473909 [debug] [MainThread]: Command end result
[0m17:44:33.514418 [info ] [MainThread]: 
[0m17:44:33.514809 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m17:44:33.515031 [info ] [MainThread]: 
[0m17:44:33.515357 [error] [MainThread]:   Database Error in model integrated_business_accounts (models/business_accounts/integrated/integrated_business_accounts.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '{'. SQLSTATE: 42601 (line 7, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.integrated_business_accounts"} */
  create or replace view `hive_metastore`.`default`.`integrated_business_accounts`
    
    
    
    as
      {
  ----^^^
      config(
          materialized='view',
          unique_key='account_id'
      )
  }
  
  with deduplicated_accounts as (
      select
          account_id,
          max(account_name) as account_name,
          max(contact_email) as contact_email,
          max(registration_date) as registration_date
      from `hive_metastore`.`default`.`staging_business_accounts`
      group by account_id
  ),
  transaction_totals as (
      select
          account_id,
          sum(transaction_amount) as total_transactions_amount
      from `hive_metastore`.`raw`.`raw_transactions`
      group by account_id
  ),
  enriched_accounts as (
      select
          a.account_id,
          a.account_name,
          a.contact_email,
          a.registration_date,
          coalesce(t.total_transactions_amount, 0) as total_transactions_amount
      from deduplicated_accounts a
      left join transaction_totals t
          on a.account_id = t.account_id
  )
  
  select
      account_id,
      account_name,
      contact_email,
      registration_date,
      total_transactions_amount
  from enriched_accounts
  order by registration_date desc
  
  compiled code at target/run/business_accounts/models/business_accounts/integrated/integrated_business_accounts.sql
[0m17:44:33.515708 [info ] [MainThread]: 
[0m17:44:33.515955 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m17:44:33.517777 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 17.004957, "process_user_time": 2.203405, "process_kernel_time": 2.345552, "process_mem_max_rss": "223412224", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:44:33.518141 [debug] [MainThread]: Command `dbt run` failed at 17:44:33.518082 after 17.01 seconds
[0m17:44:33.518412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10666bf10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069f32b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f3aee0>]}
[0m17:44:33.518670 [debug] [MainThread]: Flushing usage events
[0m17:44:55.891714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102757f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1041f85b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1041f8100>]}


============================== 17:44:55.894660 | 901e7523-1e66-4215-9d4e-5ab82f44b13b ==============================
[0m17:44:55.894660 [info ] [MainThread]: Running with dbt=1.8.7
[0m17:44:55.895002 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/logs', 'debug': 'False', 'profiles_dir': '/Users/ankurchopra/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:44:55.933423 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:44:55.933840 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:44:55.934032 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:44:56.656755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '901e7523-1e66-4215-9d4e-5ab82f44b13b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11eb44610>]}
[0m17:44:56.684680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '901e7523-1e66-4215-9d4e-5ab82f44b13b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10419af40>]}
[0m17:44:56.685377 [info ] [MainThread]: Registered adapter: databricks=1.8.7
[0m17:44:56.700757 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m17:44:56.778784 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m17:44:56.779186 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/prepared/prepared_business_accounts.sql
[0m17:44:56.899188 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.business_accounts.integrated
- models.business_accounts.staging
- models.business_accounts.prepared
[0m17:44:56.906989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '901e7523-1e66-4215-9d4e-5ab82f44b13b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e859130>]}
[0m17:44:56.968016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '901e7523-1e66-4215-9d4e-5ab82f44b13b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12d7827c0>]}
[0m17:44:56.968388 [info ] [MainThread]: Found 3 models, 2 sources, 603 macros
[0m17:44:56.968592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '901e7523-1e66-4215-9d4e-5ab82f44b13b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e8cd5b0>]}
[0m17:44:56.969611 [info ] [MainThread]: 
[0m17:44:56.970004 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5052928064, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(23710, 8635354944), compute-name=) - Creating connection
[0m17:44:56.970189 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:44:56.970368 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5052928064, session-id=None, name=master, idle-time=1.9073486328125e-06s, acquire-count=1, language=None, thread-identifier=(23710, 8635354944), compute-name=) - Acquired connection on thread (23710, 8635354944), using default compute resource
[0m17:44:56.973992 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(23710, 6207025152), compute-name=) - Creating connection
[0m17:44:56.974228 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m17:44:56.974413 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=None, name=list_hive_metastore, idle-time=1.9073486328125e-06s, acquire-count=1, language=None, thread-identifier=(23710, 6207025152), compute-name=) - Acquired connection on thread (23710, 6207025152), using default compute resource
[0m17:44:56.974615 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=None, name=list_hive_metastore, idle-time=0.00020694732666015625s, acquire-count=1, language=None, thread-identifier=(23710, 6207025152), compute-name=) - Checking idleness
[0m17:44:56.974783 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=None, name=list_hive_metastore, idle-time=0.0003769397735595703s, acquire-count=1, language=None, thread-identifier=(23710, 6207025152), compute-name=) - Retrieving connection
[0m17:44:56.974928 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m17:44:56.975088 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m17:44:56.975233 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:44:57.368441 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, name=list_hive_metastore, idle-time=1.4066696166992188e-05s, acquire-count=1, language=None, thread-identifier=(23710, 6207025152), compute-name=) - Connection created
[0m17:44:57.369813 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, command-id=Unknown) - Created cursor
[0m17:44:57.549767 [debug] [ThreadPool]: SQL status: OK in 0.570 seconds
[0m17:44:57.554032 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, command-id=01ef9963-b41c-1df0-a212-ce5c72b2ead4) - Closing cursor
[0m17:44:57.554652 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, name=list_hive_metastore, idle-time=6.9141387939453125e-06s, acquire-count=0, language=None, thread-identifier=(23710, 6207025152), compute-name=) - Released connection
[0m17:44:57.556295 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, name=list_hive_metastore, idle-time=0.0016481876373291016s, acquire-count=0, language=None, thread-identifier=(23710, 6207025152), compute-name=) - Checking idleness
[0m17:44:57.556843 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m17:44:57.557256 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, name=list_hive_metastore_default, idle-time=0.002624988555908203s, acquire-count=0, language=None, thread-identifier=(23710, 6207025152), compute-name=) - Reusing connection previously named list_hive_metastore
[0m17:44:57.557648 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, name=list_hive_metastore_default, idle-time=0.003020048141479492s, acquire-count=1, language=None, thread-identifier=(23710, 6207025152), compute-name=) - Acquired connection on thread (23710, 6207025152), using default compute resource
[0m17:44:57.558015 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, name=list_hive_metastore_default, idle-time=0.003407001495361328s, acquire-count=1, language=None, thread-identifier=(23710, 6207025152), compute-name=) - Checking idleness
[0m17:44:57.558321 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, name=list_hive_metastore_default, idle-time=0.003715038299560547s, acquire-count=1, language=None, thread-identifier=(23710, 6207025152), compute-name=) - Retrieving connection
[0m17:44:57.558596 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:44:57.558883 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m17:44:57.559201 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, command-id=Unknown) - Created cursor
[0m17:44:57.833755 [debug] [ThreadPool]: SQL status: OK in 0.270 seconds
[0m17:44:57.837881 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, command-id=01ef9963-b439-1c18-8e1c-2801719efdcd) - Closing cursor
[0m17:44:57.849658 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, name=list_hive_metastore_default, idle-time=0.29497385025024414s, acquire-count=1, language=None, thread-identifier=(23710, 6207025152), compute-name=) - Checking idleness
[0m17:44:57.850223 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, name=list_hive_metastore_default, idle-time=0.295612096786499s, acquire-count=1, language=None, thread-identifier=(23710, 6207025152), compute-name=) - Retrieving connection
[0m17:44:57.850524 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, name=list_hive_metastore_default, idle-time=0.29592108726501465s, acquire-count=1, language=None, thread-identifier=(23710, 6207025152), compute-name=) - Checking idleness
[0m17:44:57.850806 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, name=list_hive_metastore_default, idle-time=0.2962069511413574s, acquire-count=1, language=None, thread-identifier=(23710, 6207025152), compute-name=) - Retrieving connection
[0m17:44:57.851057 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:44:57.851293 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:44:57.851553 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m17:44:57.851827 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, command-id=Unknown) - Created cursor
[0m17:44:58.527375 [debug] [ThreadPool]: SQL status: OK in 0.680 seconds
[0m17:44:58.530845 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, command-id=01ef9963-b466-11c2-9607-4503610a9b75) - Closing cursor
[0m17:44:58.538740 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, name=list_hive_metastore_default, idle-time=0.9840271472930908s, acquire-count=1, language=None, thread-identifier=(23710, 6207025152), compute-name=) - Checking idleness
[0m17:44:58.539310 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, name=list_hive_metastore_default, idle-time=0.9846780300140381s, acquire-count=1, language=None, thread-identifier=(23710, 6207025152), compute-name=) - Retrieving connection
[0m17:44:58.539656 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:44:58.540015 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m17:44:58.540352 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, command-id=Unknown) - Created cursor
[0m17:44:58.924374 [debug] [ThreadPool]: SQL status: OK in 0.380 seconds
[0m17:44:58.929082 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, command-id=01ef9963-b4cf-144b-a2b1-438f7bfd2f27) - Closing cursor
[0m17:44:58.930075 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, name=list_hive_metastore_default, idle-time=4.0531158447265625e-06s, acquire-count=0, language=None, thread-identifier=(23710, 6207025152), compute-name=) - Released connection
[0m17:44:58.932237 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '901e7523-1e66-4215-9d4e-5ab82f44b13b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11eeda9a0>]}
[0m17:44:58.932798 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5052928064, session-id=None, name=master, idle-time=1.9623949527740479s, acquire-count=1, language=None, thread-identifier=(23710, 8635354944), compute-name=) - Checking idleness
[0m17:44:58.933178 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5052928064, session-id=None, name=master, idle-time=1.962785005569458s, acquire-count=1, language=None, thread-identifier=(23710, 8635354944), compute-name=) - Retrieving connection
[0m17:44:58.933527 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5052928064, session-id=None, name=master, idle-time=1.9631400108337402s, acquire-count=1, language=None, thread-identifier=(23710, 8635354944), compute-name=) - Checking idleness
[0m17:44:58.933865 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5052928064, session-id=None, name=master, idle-time=1.963479995727539s, acquire-count=1, language=None, thread-identifier=(23710, 8635354944), compute-name=) - Retrieving connection
[0m17:44:58.934195 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:44:58.934497 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:44:58.934835 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5052928064, session-id=None, name=master, idle-time=2.1457672119140625e-06s, acquire-count=0, language=None, thread-identifier=(23710, 8635354944), compute-name=) - Released connection
[0m17:44:58.935284 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m17:44:58.935681 [info ] [MainThread]: 
[0m17:44:58.939242 [debug] [Thread-1  ]: Began running node model.business_accounts.staging_business_accounts
[0m17:44:58.939888 [info ] [Thread-1  ]: 1 of 3 START sql view model default.staging_business_accounts .................. [RUN]
[0m17:44:58.940469 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, name=list_hive_metastore_default, idle-time=0.010368108749389648s, acquire-count=0, language=None, thread-identifier=(23710, 6207025152), compute-name=) - Checking idleness
[0m17:44:58.940806 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.business_accounts.staging_business_accounts)
[0m17:44:58.941190 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, name=model.business_accounts.staging_business_accounts, idle-time=0.011101961135864258s, acquire-count=0, language=None, thread-identifier=(23710, 6207025152), compute-name=) - Reusing connection previously named list_hive_metastore_default
[0m17:44:58.941562 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, name=model.business_accounts.staging_business_accounts, idle-time=0.011475086212158203s, acquire-count=1, language=sql, thread-identifier=(23710, 6207025152), compute-name=) - Acquired connection on thread (23710, 6207025152), using default compute resource for model '`hive_metastore`.`default`.`staging_business_accounts`'
[0m17:44:58.941910 [debug] [Thread-1  ]: Began compiling node model.business_accounts.staging_business_accounts
[0m17:44:58.950830 [debug] [Thread-1  ]: Writing injected SQL for node "model.business_accounts.staging_business_accounts"
[0m17:44:58.951466 [debug] [Thread-1  ]: Began executing node model.business_accounts.staging_business_accounts
[0m17:44:58.972629 [debug] [Thread-1  ]: Writing runtime sql for node "model.business_accounts.staging_business_accounts"
[0m17:44:58.973301 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, name=model.business_accounts.staging_business_accounts, idle-time=0.04323101043701172s, acquire-count=1, language=sql, thread-identifier=(23710, 6207025152), compute-name=) - Checking idleness
[0m17:44:58.973570 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, name=model.business_accounts.staging_business_accounts, idle-time=0.04352593421936035s, acquire-count=1, language=sql, thread-identifier=(23710, 6207025152), compute-name=) - Retrieving connection
[0m17:44:58.973769 [debug] [Thread-1  ]: Using databricks connection "model.business_accounts.staging_business_accounts"
[0m17:44:58.974024 [debug] [Thread-1  ]: On model.business_accounts.staging_business_accounts: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.staging_business_accounts"} */
create or replace view `hive_metastore`.`default`.`staging_business_accounts`
  
  
  
  as
    

with staging_business_accounts as (
    select
        account_id,
        business_name as account_name,
        contact_email,
        to_date(registration_date, 'YYYY-MM-DD') as registration_date
    from `hive_metastore`.`raw`.`raw_business_accounts`
    where contact_email is not null
)

select *
from staging_business_accounts;

[0m17:44:58.974292 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, command-id=Unknown) - Created cursor
[0m17:44:59.757532 [debug] [Thread-1  ]: SQL status: OK in 0.780 seconds
[0m17:44:59.759284 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, command-id=01ef9963-b511-1ac4-988a-5384a54f7fa2) - Closing cursor
[0m17:44:59.778595 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, name=model.business_accounts.staging_business_accounts, idle-time=4.0531158447265625e-06s, acquire-count=0, language=sql, thread-identifier=(23710, 6207025152), compute-name=) - Released connection
[0m17:44:59.779147 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5075834624, session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3, name=model.business_accounts.staging_business_accounts, idle-time=2.1457672119140625e-06s, acquire-count=0, language=sql, thread-identifier=(23710, 6207025152), compute-name=) - Released connection
[0m17:44:59.780357 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '901e7523-1e66-4215-9d4e-5ab82f44b13b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12d733700>]}
[0m17:44:59.780910 [info ] [Thread-1  ]: 1 of 3 OK created sql view model default.staging_business_accounts ............. [[32mOK[0m in 0.84s]
[0m17:44:59.781437 [debug] [Thread-1  ]: Finished running node model.business_accounts.staging_business_accounts
[0m17:44:59.782035 [debug] [Thread-3  ]: Began running node model.business_accounts.integrated_business_accounts
[0m17:44:59.782593 [info ] [Thread-3  ]: 2 of 3 START sql view model default.integrated_business_accounts ............... [RUN]
[0m17:44:59.783167 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5058073120, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0s, acquire-count=0, language=None, thread-identifier=(23710, 6247559168), compute-name=) - Creating connection
[0m17:44:59.783569 [debug] [Thread-3  ]: Acquiring new databricks connection 'model.business_accounts.integrated_business_accounts'
[0m17:44:59.783932 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5058073120, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=2.6226043701171875e-06s, acquire-count=1, language=sql, thread-identifier=(23710, 6247559168), compute-name=) - Acquired connection on thread (23710, 6247559168), using default compute resource for model '`hive_metastore`.`default`.`integrated_business_accounts`'
[0m17:44:59.784254 [debug] [Thread-3  ]: Began compiling node model.business_accounts.integrated_business_accounts
[0m17:44:59.787811 [debug] [Thread-3  ]: Writing injected SQL for node "model.business_accounts.integrated_business_accounts"
[0m17:44:59.788505 [debug] [Thread-3  ]: Began executing node model.business_accounts.integrated_business_accounts
[0m17:44:59.791269 [debug] [Thread-3  ]: Writing runtime sql for node "model.business_accounts.integrated_business_accounts"
[0m17:44:59.791812 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5058073120, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0.007889747619628906s, acquire-count=1, language=sql, thread-identifier=(23710, 6247559168), compute-name=) - Checking idleness
[0m17:44:59.792116 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5058073120, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0.008211851119995117s, acquire-count=1, language=sql, thread-identifier=(23710, 6247559168), compute-name=) - Retrieving connection
[0m17:44:59.792410 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5058073120, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0.008507728576660156s, acquire-count=1, language=sql, thread-identifier=(23710, 6247559168), compute-name=) - Checking idleness
[0m17:44:59.792686 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5058073120, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0.008788824081420898s, acquire-count=1, language=sql, thread-identifier=(23710, 6247559168), compute-name=) - Retrieving connection
[0m17:44:59.792945 [debug] [Thread-3  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:44:59.793143 [debug] [Thread-3  ]: Using databricks connection "model.business_accounts.integrated_business_accounts"
[0m17:44:59.793486 [debug] [Thread-3  ]: On model.business_accounts.integrated_business_accounts: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.integrated_business_accounts"} */
create or replace view `hive_metastore`.`default`.`integrated_business_accounts`
  
  
  
  as
    {
    config(
        materialized='view',
        unique_key='account_id'
    )
}

with deduplicated_accounts as (
    select
        account_id,
        max(account_name) as account_name,
        max(contact_email) as contact_email,
        max(registration_date) as registration_date
    from `hive_metastore`.`default`.`staging_business_accounts`
    group by account_id
),
transaction_totals as (
    select
        account_id,
        sum(transaction_amount) as total_transactions_amount
    from `hive_metastore`.`raw`.`raw_transactions`
    group by account_id
),
enriched_accounts as (
    select
        a.account_id,
        a.account_name,
        a.contact_email,
        a.registration_date,
        coalesce(t.total_transactions_amount, 0) as total_transactions_amount
    from deduplicated_accounts a
    left join transaction_totals t
        on a.account_id = t.account_id
)

select
    account_id,
    account_name,
    contact_email,
    registration_date,
    total_transactions_amount
from enriched_accounts
order by registration_date desc;

[0m17:44:59.793784 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m17:45:00.301226 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5058073120, session-id=01ef9963-b5b2-1628-a505-d60c014242dc, name=model.business_accounts.integrated_business_accounts, idle-time=1.1205673217773438e-05s, acquire-count=1, language=sql, thread-identifier=(23710, 6247559168), compute-name=) - Connection created
[0m17:45:00.302055 [debug] [Thread-3  ]: Databricks adapter: Cursor(session-id=01ef9963-b5b2-1628-a505-d60c014242dc, command-id=Unknown) - Created cursor
[0m17:45:00.551645 [debug] [Thread-3  ]: Databricks adapter: Cursor(session-id=01ef9963-b5b2-1628-a505-d60c014242dc, command-id=Unknown) - Closing cursor
[0m17:45:00.553537 [debug] [Thread-3  ]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.integrated_business_accounts"} */
create or replace view `hive_metastore`.`default`.`integrated_business_accounts`
  
  
  
  as
    {
    config(
        materialized='view',
        unique_key='account_id'
    )
}

with deduplicated_accounts as (
    select
        account_id,
        max(account_name) as account_name,
        max(contact_email) as contact_email,
        max(registration_date) as registration_date
    from `hive_metastore`.`default`.`staging_business_accounts`
    group by account_id
),
transaction_totals as (
    select
        account_id,
        sum(transaction_amount) as total_transactions_amount
    from `hive_metastore`.`raw`.`raw_transactions`
    group by account_id
),
enriched_accounts as (
    select
        a.account_id,
        a.account_name,
        a.contact_email,
        a.registration_date,
        coalesce(t.total_transactions_amount, 0) as total_transactions_amount
    from deduplicated_accounts a
    left join transaction_totals t
        on a.account_id = t.account_id
)

select
    account_id,
    account_name,
    contact_email,
    registration_date,
    total_transactions_amount
from enriched_accounts
order by registration_date desc;

: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '{'. SQLSTATE: 42601 (line 7, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.integrated_business_accounts"} */
create or replace view `hive_metastore`.`default`.`integrated_business_accounts`
  
  
  
  as
    {
----^^^
    config(
        materialized='view',
        unique_key='account_id'
    )
}

with deduplicated_accounts as (
    select
        account_id,
        max(account_name) as account_name,
        max(contact_email) as contact_email,
        max(registration_date) as registration_date
    from `hive_metastore`.`default`.`staging_business_accounts`
    group by account_id
),
transaction_totals as (
    select
        account_id,
        sum(transaction_amount) as total_transactions_amount
    from `hive_metastore`.`raw`.`raw_transactions`
    group by account_id
),
enriched_accounts as (
    select
        a.account_id,
        a.account_name,
        a.contact_email,
        a.registration_date,
        coalesce(t.total_transactions_amount, 0) as total_transactions_amount
    from deduplicated_accounts a
    left join transaction_totals t
        on a.account_id = t.account_id
)

select
    account_id,
    account_name,
    contact_email,
    registration_date,
    total_transactions_amount
from enriched_accounts
order by registration_date desc

Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '{'. SQLSTATE: 42601 (line 7, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.integrated_business_accounts"} */
create or replace view `hive_metastore`.`default`.`integrated_business_accounts`
  
  
  
  as
    {
----^^^
    config(
        materialized='view',
        unique_key='account_id'
    )
}

with deduplicated_accounts as (
    select
        account_id,
        max(account_name) as account_name,
        max(contact_email) as contact_email,
        max(registration_date) as registration_date
    from `hive_metastore`.`default`.`staging_business_accounts`
    group by account_id
),
transaction_totals as (
    select
        account_id,
        sum(transaction_amount) as total_transactions_amount
    from `hive_metastore`.`raw`.`raw_transactions`
    group by account_id
),
enriched_accounts as (
    select
        a.account_id,
        a.account_name,
        a.contact_email,
        a.registration_date,
        coalesce(t.total_transactions_amount, 0) as total_transactions_amount
    from deduplicated_accounts a
    left join transaction_totals t
        on a.account_id = t.account_id
)

select
    account_id,
    account_name,
    contact_email,
    registration_date,
    total_transactions_amount
from enriched_accounts
order by registration_date desc

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:786)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:624)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:469)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:704)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:469)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:27)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:74)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:174)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:729)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:738)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:617)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:615)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:446)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:432)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:482)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '{'. SQLSTATE: 42601 (line 7, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.integrated_business_accounts"} */
create or replace view `hive_metastore`.`default`.`integrated_business_accounts`
  
  
  
  as
    {
----^^^
    config(
        materialized='view',
        unique_key='account_id'
    )
}

with deduplicated_accounts as (
    select
        account_id,
        max(account_name) as account_name,
        max(contact_email) as contact_email,
        max(registration_date) as registration_date
    from `hive_metastore`.`default`.`staging_business_accounts`
    group by account_id
),
transaction_totals as (
    select
        account_id,
        sum(transaction_amount) as total_transactions_amount
    from `hive_metastore`.`raw`.`raw_transactions`
    group by account_id
),
enriched_accounts as (
    select
        a.account_id,
        a.account_name,
        a.contact_email,
        a.registration_date,
        coalesce(t.total_transactions_amount, 0) as total_transactions_amount
    from deduplicated_accounts a
    left join transaction_totals t
        on a.account_id = t.account_id
)

select
    account_id,
    account_name,
    contact_email,
    registration_date,
    total_transactions_amount
from enriched_accounts
order by registration_date desc

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:308)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:114)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:137)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:106)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:80)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:101)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:77)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:383)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:453)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:382)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:143)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:382)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:668)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:665)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:485)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:602)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:598)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:113)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:112)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:89)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:154)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)
	... 3 more
, operation-id=01ef9963-b5dc-128a-8773-7cd812e57fd9
[0m17:45:00.555232 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5058073120, session-id=01ef9963-b5b2-1628-a505-d60c014242dc, name=model.business_accounts.integrated_business_accounts, idle-time=8.821487426757812e-06s, acquire-count=0, language=sql, thread-identifier=(23710, 6247559168), compute-name=) - Released connection
[0m17:45:00.564625 [debug] [Thread-3  ]: Database Error in model integrated_business_accounts (models/business_accounts/integrated/integrated_business_accounts.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '{'. SQLSTATE: 42601 (line 7, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.integrated_business_accounts"} */
  create or replace view `hive_metastore`.`default`.`integrated_business_accounts`
    
    
    
    as
      {
  ----^^^
      config(
          materialized='view',
          unique_key='account_id'
      )
  }
  
  with deduplicated_accounts as (
      select
          account_id,
          max(account_name) as account_name,
          max(contact_email) as contact_email,
          max(registration_date) as registration_date
      from `hive_metastore`.`default`.`staging_business_accounts`
      group by account_id
  ),
  transaction_totals as (
      select
          account_id,
          sum(transaction_amount) as total_transactions_amount
      from `hive_metastore`.`raw`.`raw_transactions`
      group by account_id
  ),
  enriched_accounts as (
      select
          a.account_id,
          a.account_name,
          a.contact_email,
          a.registration_date,
          coalesce(t.total_transactions_amount, 0) as total_transactions_amount
      from deduplicated_accounts a
      left join transaction_totals t
          on a.account_id = t.account_id
  )
  
  select
      account_id,
      account_name,
      contact_email,
      registration_date,
      total_transactions_amount
  from enriched_accounts
  order by registration_date desc
  
  compiled code at target/run/business_accounts/models/business_accounts/integrated/integrated_business_accounts.sql
[0m17:45:00.565419 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5058073120, session-id=01ef9963-b5b2-1628-a505-d60c014242dc, name=model.business_accounts.integrated_business_accounts, idle-time=2.86102294921875e-06s, acquire-count=0, language=sql, thread-identifier=(23710, 6247559168), compute-name=) - Released connection
[0m17:45:00.565936 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '901e7523-1e66-4215-9d4e-5ab82f44b13b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e8ca7c0>]}
[0m17:45:00.566708 [error] [Thread-3  ]: 2 of 3 ERROR creating sql view model default.integrated_business_accounts ...... [[31mERROR[0m in 0.78s]
[0m17:45:00.567395 [debug] [Thread-3  ]: Finished running node model.business_accounts.integrated_business_accounts
[0m17:45:00.568204 [debug] [Thread-2  ]: Began running node model.business_accounts.prepared_business_accounts
[0m17:45:00.568708 [info ] [Thread-2  ]: 3 of 3 SKIP relation default.prepared_business_accounts ........................ [[33mSKIP[0m]
[0m17:45:00.569117 [debug] [Thread-2  ]: Finished running node model.business_accounts.prepared_business_accounts
[0m17:45:00.570118 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5052928064, session-id=None, name=master, idle-time=1.6352591514587402s, acquire-count=0, language=None, thread-identifier=(23710, 8635354944), compute-name=) - Checking idleness
[0m17:45:00.570531 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5052928064, session-id=None, name=master, idle-time=1.635693073272705s, acquire-count=0, language=None, thread-identifier=(23710, 8635354944), compute-name=) - Reusing connection previously named master
[0m17:45:00.570855 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5052928064, session-id=None, name=master, idle-time=1.6360220909118652s, acquire-count=1, language=None, thread-identifier=(23710, 8635354944), compute-name=) - Acquired connection on thread (23710, 8635354944), using default compute resource
[0m17:45:00.571183 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5052928064, session-id=None, name=master, idle-time=1.6363580226898193s, acquire-count=1, language=None, thread-identifier=(23710, 8635354944), compute-name=) - Checking idleness
[0m17:45:00.571480 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5052928064, session-id=None, name=master, idle-time=1.6366541385650635s, acquire-count=1, language=None, thread-identifier=(23710, 8635354944), compute-name=) - Retrieving connection
[0m17:45:00.571754 [debug] [MainThread]: On master: ROLLBACK
[0m17:45:00.572016 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:45:00.896672 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5052928064, session-id=01ef9963-b626-1b97-a2f7-bb7fa358d2d3, name=master, idle-time=2.09808349609375e-05s, acquire-count=1, language=None, thread-identifier=(23710, 8635354944), compute-name=) - Connection created
[0m17:45:00.898125 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:45:00.898960 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5052928064, session-id=01ef9963-b626-1b97-a2f7-bb7fa358d2d3, name=master, idle-time=0.0026476383209228516s, acquire-count=1, language=None, thread-identifier=(23710, 8635354944), compute-name=) - Checking idleness
[0m17:45:00.899511 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5052928064, session-id=01ef9963-b626-1b97-a2f7-bb7fa358d2d3, name=master, idle-time=0.0032210350036621094s, acquire-count=1, language=None, thread-identifier=(23710, 8635354944), compute-name=) - Retrieving connection
[0m17:45:00.900004 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:45:00.900456 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:45:00.900965 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5052928064, session-id=01ef9963-b626-1b97-a2f7-bb7fa358d2d3, name=master, idle-time=3.0994415283203125e-06s, acquire-count=0, language=None, thread-identifier=(23710, 8635354944), compute-name=) - Released connection
[0m17:45:00.901707 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:45:00.902118 [debug] [MainThread]: On master: ROLLBACK
[0m17:45:00.902483 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:45:00.902838 [debug] [MainThread]: On master: Close
[0m17:45:00.903227 [debug] [MainThread]: Databricks adapter: Connection(session-id=01ef9963-b626-1b97-a2f7-bb7fa358d2d3) - Closing connection
[0m17:45:00.999793 [debug] [MainThread]: Connection 'model.business_accounts.staging_business_accounts' was properly closed.
[0m17:45:01.001172 [debug] [MainThread]: On model.business_accounts.staging_business_accounts: ROLLBACK
[0m17:45:01.001780 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:45:01.002241 [debug] [MainThread]: On model.business_accounts.staging_business_accounts: Close
[0m17:45:01.002757 [debug] [MainThread]: Databricks adapter: Connection(session-id=01ef9963-b40a-12e3-a8f9-57ba79911fe3) - Closing connection
[0m17:45:01.098705 [debug] [MainThread]: Connection 'model.business_accounts.integrated_business_accounts' was properly closed.
[0m17:45:01.099420 [debug] [MainThread]: On model.business_accounts.integrated_business_accounts: ROLLBACK
[0m17:45:01.099928 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:45:01.100376 [debug] [MainThread]: On model.business_accounts.integrated_business_accounts: Close
[0m17:45:01.100865 [debug] [MainThread]: Databricks adapter: Connection(session-id=01ef9963-b5b2-1628-a505-d60c014242dc) - Closing connection
[0m17:45:01.205800 [info ] [MainThread]: 
[0m17:45:01.206577 [info ] [MainThread]: Finished running 2 view models, 1 table model in 0 hours 0 minutes and 4.24 seconds (4.24s).
[0m17:45:01.208046 [debug] [MainThread]: Command end result
[0m17:45:01.248090 [info ] [MainThread]: 
[0m17:45:01.248451 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m17:45:01.248664 [info ] [MainThread]: 
[0m17:45:01.248991 [error] [MainThread]:   Database Error in model integrated_business_accounts (models/business_accounts/integrated/integrated_business_accounts.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '{'. SQLSTATE: 42601 (line 7, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.integrated_business_accounts"} */
  create or replace view `hive_metastore`.`default`.`integrated_business_accounts`
    
    
    
    as
      {
  ----^^^
      config(
          materialized='view',
          unique_key='account_id'
      )
  }
  
  with deduplicated_accounts as (
      select
          account_id,
          max(account_name) as account_name,
          max(contact_email) as contact_email,
          max(registration_date) as registration_date
      from `hive_metastore`.`default`.`staging_business_accounts`
      group by account_id
  ),
  transaction_totals as (
      select
          account_id,
          sum(transaction_amount) as total_transactions_amount
      from `hive_metastore`.`raw`.`raw_transactions`
      group by account_id
  ),
  enriched_accounts as (
      select
          a.account_id,
          a.account_name,
          a.contact_email,
          a.registration_date,
          coalesce(t.total_transactions_amount, 0) as total_transactions_amount
      from deduplicated_accounts a
      left join transaction_totals t
          on a.account_id = t.account_id
  )
  
  select
      account_id,
      account_name,
      contact_email,
      registration_date,
      total_transactions_amount
  from enriched_accounts
  order by registration_date desc
  
  compiled code at target/run/business_accounts/models/business_accounts/integrated/integrated_business_accounts.sql
[0m17:45:01.249282 [info ] [MainThread]: 
[0m17:45:01.249496 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=1 TOTAL=3
[0m17:45:01.251429 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 5.3964925, "process_user_time": 2.188559, "process_kernel_time": 2.322928, "process_mem_max_rss": "223526912", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:45:01.251966 [debug] [MainThread]: Command `dbt run` failed at 17:45:01.251898 after 5.40 seconds
[0m17:45:01.252261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102757f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11edf36a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10402a0a0>]}
[0m17:45:01.252535 [debug] [MainThread]: Flushing usage events
[0m17:45:14.710116 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102b23f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057ff5b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057ff100>]}


============================== 17:45:14.713126 | 7867f181-226b-4757-9d38-12bb29aa2b35 ==============================
[0m17:45:14.713126 [info ] [MainThread]: Running with dbt=1.8.7
[0m17:45:14.713464 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/ankurchopra/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:45:14.753958 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:45:14.754322 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:45:14.754514 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:45:15.528673 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7867f181-226b-4757-9d38-12bb29aa2b35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f358610>]}
[0m17:45:15.557055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7867f181-226b-4757-9d38-12bb29aa2b35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057a2f40>]}
[0m17:45:15.557541 [info ] [MainThread]: Registered adapter: databricks=1.8.7
[0m17:45:15.572818 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m17:45:15.654997 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m17:45:15.655495 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/integrated/integrated_business_accounts.sql
[0m17:45:15.778996 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.business_accounts.integrated
- models.business_accounts.prepared
- models.business_accounts.staging
[0m17:45:15.787760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7867f181-226b-4757-9d38-12bb29aa2b35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x146a4b130>]}
[0m17:45:15.848519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7867f181-226b-4757-9d38-12bb29aa2b35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x146a4c4f0>]}
[0m17:45:15.848852 [info ] [MainThread]: Found 3 models, 2 sources, 603 macros
[0m17:45:15.849054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7867f181-226b-4757-9d38-12bb29aa2b35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x146a6af70>]}
[0m17:45:15.849906 [info ] [MainThread]: 
[0m17:45:15.850301 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5480171840, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(23721, 8635354944), compute-name=) - Creating connection
[0m17:45:15.850486 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:45:15.850653 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5480171840, session-id=None, name=master, idle-time=9.5367431640625e-07s, acquire-count=1, language=None, thread-identifier=(23721, 8635354944), compute-name=) - Acquired connection on thread (23721, 8635354944), using default compute resource
[0m17:45:15.854186 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(23721, 6203043840), compute-name=) - Creating connection
[0m17:45:15.854410 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m17:45:15.854584 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=None, name=list_hive_metastore, idle-time=2.1457672119140625e-06s, acquire-count=1, language=None, thread-identifier=(23721, 6203043840), compute-name=) - Acquired connection on thread (23721, 6203043840), using default compute resource
[0m17:45:15.854775 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=None, name=list_hive_metastore, idle-time=0.0001971721649169922s, acquire-count=1, language=None, thread-identifier=(23721, 6203043840), compute-name=) - Checking idleness
[0m17:45:15.854938 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=None, name=list_hive_metastore, idle-time=0.00036025047302246094s, acquire-count=1, language=None, thread-identifier=(23721, 6203043840), compute-name=) - Retrieving connection
[0m17:45:15.855088 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m17:45:15.855242 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m17:45:15.855388 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:45:16.189687 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, name=list_hive_metastore, idle-time=6.198883056640625e-06s, acquire-count=1, language=None, thread-identifier=(23721, 6203043840), compute-name=) - Connection created
[0m17:45:16.190615 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, command-id=Unknown) - Created cursor
[0m17:45:16.358131 [debug] [ThreadPool]: SQL status: OK in 0.500 seconds
[0m17:45:16.363810 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, command-id=01ef9963-bf54-10b3-96ee-7e826d4e4df5) - Closing cursor
[0m17:45:16.364481 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, name=list_hive_metastore, idle-time=5.9604644775390625e-06s, acquire-count=0, language=None, thread-identifier=(23721, 6203043840), compute-name=) - Released connection
[0m17:45:16.366076 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, name=list_hive_metastore, idle-time=0.0016071796417236328s, acquire-count=0, language=None, thread-identifier=(23721, 6203043840), compute-name=) - Checking idleness
[0m17:45:16.366521 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m17:45:16.366968 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, name=list_hive_metastore_default, idle-time=0.0024979114532470703s, acquire-count=0, language=None, thread-identifier=(23721, 6203043840), compute-name=) - Reusing connection previously named list_hive_metastore
[0m17:45:16.367309 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, name=list_hive_metastore_default, idle-time=0.0028600692749023438s, acquire-count=1, language=None, thread-identifier=(23721, 6203043840), compute-name=) - Acquired connection on thread (23721, 6203043840), using default compute resource
[0m17:45:16.367643 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, name=list_hive_metastore_default, idle-time=0.003198862075805664s, acquire-count=1, language=None, thread-identifier=(23721, 6203043840), compute-name=) - Checking idleness
[0m17:45:16.367953 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, name=list_hive_metastore_default, idle-time=0.003504037857055664s, acquire-count=1, language=None, thread-identifier=(23721, 6203043840), compute-name=) - Retrieving connection
[0m17:45:16.368222 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:45:16.368505 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m17:45:16.368800 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, command-id=Unknown) - Created cursor
[0m17:45:16.568094 [debug] [ThreadPool]: SQL status: OK in 0.200 seconds
[0m17:45:16.571976 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, command-id=01ef9963-bf6f-1779-9675-ce0820ff8287) - Closing cursor
[0m17:45:16.642311 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, name=list_hive_metastore_default, idle-time=0.2778449058532715s, acquire-count=1, language=None, thread-identifier=(23721, 6203043840), compute-name=) - Checking idleness
[0m17:45:16.642635 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, name=list_hive_metastore_default, idle-time=0.2782130241394043s, acquire-count=1, language=None, thread-identifier=(23721, 6203043840), compute-name=) - Retrieving connection
[0m17:45:16.642825 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, name=list_hive_metastore_default, idle-time=0.2784099578857422s, acquire-count=1, language=None, thread-identifier=(23721, 6203043840), compute-name=) - Checking idleness
[0m17:45:16.642996 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, name=list_hive_metastore_default, idle-time=0.278580904006958s, acquire-count=1, language=None, thread-identifier=(23721, 6203043840), compute-name=) - Retrieving connection
[0m17:45:16.643149 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:45:16.643293 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:45:16.643453 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m17:45:16.643635 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, command-id=Unknown) - Created cursor
[0m17:45:16.854358 [debug] [ThreadPool]: SQL status: OK in 0.210 seconds
[0m17:45:16.857251 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, command-id=01ef9963-bf99-1531-abd7-079d27aa0239) - Closing cursor
[0m17:45:16.864257 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, name=list_hive_metastore_default, idle-time=0.49977803230285645s, acquire-count=1, language=None, thread-identifier=(23721, 6203043840), compute-name=) - Checking idleness
[0m17:45:16.864671 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, name=list_hive_metastore_default, idle-time=0.5002200603485107s, acquire-count=1, language=None, thread-identifier=(23721, 6203043840), compute-name=) - Retrieving connection
[0m17:45:16.864973 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:45:16.865282 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m17:45:16.865609 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, command-id=Unknown) - Created cursor
[0m17:45:17.127732 [debug] [ThreadPool]: SQL status: OK in 0.260 seconds
[0m17:45:17.132183 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, command-id=01ef9963-bfba-1dab-a622-c06e891b1743) - Closing cursor
[0m17:45:17.133243 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, name=list_hive_metastore_default, idle-time=6.198883056640625e-06s, acquire-count=0, language=None, thread-identifier=(23721, 6203043840), compute-name=) - Released connection
[0m17:45:17.135490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7867f181-226b-4757-9d38-12bb29aa2b35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10217aa30>]}
[0m17:45:17.136089 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5480171840, session-id=None, name=master, idle-time=1.2853949069976807s, acquire-count=1, language=None, thread-identifier=(23721, 8635354944), compute-name=) - Checking idleness
[0m17:45:17.136452 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5480171840, session-id=None, name=master, idle-time=1.2857708930969238s, acquire-count=1, language=None, thread-identifier=(23721, 8635354944), compute-name=) - Retrieving connection
[0m17:45:17.136804 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5480171840, session-id=None, name=master, idle-time=1.2861270904541016s, acquire-count=1, language=None, thread-identifier=(23721, 8635354944), compute-name=) - Checking idleness
[0m17:45:17.137137 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5480171840, session-id=None, name=master, idle-time=1.2864587306976318s, acquire-count=1, language=None, thread-identifier=(23721, 8635354944), compute-name=) - Retrieving connection
[0m17:45:17.137453 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:45:17.137777 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:45:17.138106 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5480171840, session-id=None, name=master, idle-time=2.1457672119140625e-06s, acquire-count=0, language=None, thread-identifier=(23721, 8635354944), compute-name=) - Released connection
[0m17:45:17.138559 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m17:45:17.138930 [info ] [MainThread]: 
[0m17:45:17.142066 [debug] [Thread-1  ]: Began running node model.business_accounts.staging_business_accounts
[0m17:45:17.142682 [info ] [Thread-1  ]: 1 of 3 START sql view model default.staging_business_accounts .................. [RUN]
[0m17:45:17.143245 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, name=list_hive_metastore_default, idle-time=0.009984016418457031s, acquire-count=0, language=None, thread-identifier=(23721, 6203043840), compute-name=) - Checking idleness
[0m17:45:17.143578 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.business_accounts.staging_business_accounts)
[0m17:45:17.143948 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, name=model.business_accounts.staging_business_accounts, idle-time=0.010701179504394531s, acquire-count=0, language=None, thread-identifier=(23721, 6203043840), compute-name=) - Reusing connection previously named list_hive_metastore_default
[0m17:45:17.144318 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, name=model.business_accounts.staging_business_accounts, idle-time=0.011073112487792969s, acquire-count=1, language=sql, thread-identifier=(23721, 6203043840), compute-name=) - Acquired connection on thread (23721, 6203043840), using default compute resource for model '`hive_metastore`.`default`.`staging_business_accounts`'
[0m17:45:17.144663 [debug] [Thread-1  ]: Began compiling node model.business_accounts.staging_business_accounts
[0m17:45:17.153694 [debug] [Thread-1  ]: Writing injected SQL for node "model.business_accounts.staging_business_accounts"
[0m17:45:17.154436 [debug] [Thread-1  ]: Began executing node model.business_accounts.staging_business_accounts
[0m17:45:17.174919 [debug] [Thread-1  ]: Writing runtime sql for node "model.business_accounts.staging_business_accounts"
[0m17:45:17.175568 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, name=model.business_accounts.staging_business_accounts, idle-time=0.04234123229980469s, acquire-count=1, language=sql, thread-identifier=(23721, 6203043840), compute-name=) - Checking idleness
[0m17:45:17.175832 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, name=model.business_accounts.staging_business_accounts, idle-time=0.042629241943359375s, acquire-count=1, language=sql, thread-identifier=(23721, 6203043840), compute-name=) - Retrieving connection
[0m17:45:17.176021 [debug] [Thread-1  ]: Using databricks connection "model.business_accounts.staging_business_accounts"
[0m17:45:17.176276 [debug] [Thread-1  ]: On model.business_accounts.staging_business_accounts: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.staging_business_accounts"} */
create or replace view `hive_metastore`.`default`.`staging_business_accounts`
  
  
  
  as
    

with staging_business_accounts as (
    select
        account_id,
        business_name as account_name,
        contact_email,
        to_date(registration_date, 'YYYY-MM-DD') as registration_date
    from `hive_metastore`.`raw`.`raw_business_accounts`
    where contact_email is not null
)

select *
from staging_business_accounts;

[0m17:45:17.176567 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, command-id=Unknown) - Created cursor
[0m17:45:17.883187 [debug] [Thread-1  ]: SQL status: OK in 0.710 seconds
[0m17:45:17.885141 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, command-id=01ef9963-bfeb-1028-a13a-1513394c5496) - Closing cursor
[0m17:45:17.906725 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, name=model.business_accounts.staging_business_accounts, idle-time=4.0531158447265625e-06s, acquire-count=0, language=sql, thread-identifier=(23721, 6203043840), compute-name=) - Released connection
[0m17:45:17.907308 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5480081296, session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3, name=model.business_accounts.staging_business_accounts, idle-time=1.6689300537109375e-06s, acquire-count=0, language=sql, thread-identifier=(23721, 6203043840), compute-name=) - Released connection
[0m17:45:17.908505 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7867f181-226b-4757-9d38-12bb29aa2b35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103a022b0>]}
[0m17:45:17.909113 [info ] [Thread-1  ]: 1 of 3 OK created sql view model default.staging_business_accounts ............. [[32mOK[0m in 0.76s]
[0m17:45:17.909653 [debug] [Thread-1  ]: Finished running node model.business_accounts.staging_business_accounts
[0m17:45:17.910305 [debug] [Thread-3  ]: Began running node model.business_accounts.integrated_business_accounts
[0m17:45:17.910850 [info ] [Thread-3  ]: 2 of 3 START sql view model default.integrated_business_accounts ............... [RUN]
[0m17:45:17.911419 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=4379848032, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0s, acquire-count=0, language=None, thread-identifier=(23721, 6243577856), compute-name=) - Creating connection
[0m17:45:17.911840 [debug] [Thread-3  ]: Acquiring new databricks connection 'model.business_accounts.integrated_business_accounts'
[0m17:45:17.912185 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=4379848032, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=3.814697265625e-06s, acquire-count=1, language=sql, thread-identifier=(23721, 6243577856), compute-name=) - Acquired connection on thread (23721, 6243577856), using default compute resource for model '`hive_metastore`.`default`.`integrated_business_accounts`'
[0m17:45:17.912511 [debug] [Thread-3  ]: Began compiling node model.business_accounts.integrated_business_accounts
[0m17:45:17.916112 [debug] [Thread-3  ]: Writing injected SQL for node "model.business_accounts.integrated_business_accounts"
[0m17:45:17.916715 [debug] [Thread-3  ]: Began executing node model.business_accounts.integrated_business_accounts
[0m17:45:17.919475 [debug] [Thread-3  ]: Writing runtime sql for node "model.business_accounts.integrated_business_accounts"
[0m17:45:17.920037 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=4379848032, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0.007858991622924805s, acquire-count=1, language=sql, thread-identifier=(23721, 6243577856), compute-name=) - Checking idleness
[0m17:45:17.920342 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=4379848032, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0.008182048797607422s, acquire-count=1, language=sql, thread-identifier=(23721, 6243577856), compute-name=) - Retrieving connection
[0m17:45:17.920637 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=4379848032, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0.008480072021484375s, acquire-count=1, language=sql, thread-identifier=(23721, 6243577856), compute-name=) - Checking idleness
[0m17:45:17.920920 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=4379848032, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0.008767127990722656s, acquire-count=1, language=sql, thread-identifier=(23721, 6243577856), compute-name=) - Retrieving connection
[0m17:45:17.921184 [debug] [Thread-3  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:45:17.921385 [debug] [Thread-3  ]: Using databricks connection "model.business_accounts.integrated_business_accounts"
[0m17:45:17.921732 [debug] [Thread-3  ]: On model.business_accounts.integrated_business_accounts: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.integrated_business_accounts"} */
create or replace view `hive_metastore`.`default`.`integrated_business_accounts`
  
  
  
  as
    

with deduplicated_accounts as (
    select
        account_id,
        max(account_name) as account_name,
        max(contact_email) as contact_email,
        max(registration_date) as registration_date
    from `hive_metastore`.`default`.`staging_business_accounts`
    group by account_id
),
transaction_totals as (
    select
        account_id,
        sum(transaction_amount) as total_transactions_amount
    from `hive_metastore`.`raw`.`raw_transactions`
    group by account_id
),
enriched_accounts as (
    select
        a.account_id,
        a.account_name,
        a.contact_email,
        a.registration_date,
        coalesce(t.total_transactions_amount, 0) as total_transactions_amount
    from deduplicated_accounts a
    left join transaction_totals t
        on a.account_id = t.account_id
)

select
    account_id,
    account_name,
    contact_email,
    registration_date,
    total_transactions_amount
from enriched_accounts
order by registration_date desc;

[0m17:45:17.922035 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m17:45:18.253109 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=4379848032, session-id=01ef9963-c07f-13f1-80dd-a96a3504b15e, name=model.business_accounts.integrated_business_accounts, idle-time=6.9141387939453125e-06s, acquire-count=1, language=sql, thread-identifier=(23721, 6243577856), compute-name=) - Connection created
[0m17:45:18.254277 [debug] [Thread-3  ]: Databricks adapter: Cursor(session-id=01ef9963-c07f-13f1-80dd-a96a3504b15e, command-id=Unknown) - Created cursor
[0m17:45:19.214882 [debug] [Thread-3  ]: SQL status: OK in 1.290 seconds
[0m17:45:19.216999 [debug] [Thread-3  ]: Databricks adapter: Cursor(session-id=01ef9963-c07f-13f1-80dd-a96a3504b15e, command-id=01ef9963-c08e-1e9c-aaf1-ba73fdfca03d) - Closing cursor
[0m17:45:19.218839 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=4379848032, session-id=01ef9963-c07f-13f1-80dd-a96a3504b15e, name=model.business_accounts.integrated_business_accounts, idle-time=4.76837158203125e-06s, acquire-count=0, language=sql, thread-identifier=(23721, 6243577856), compute-name=) - Released connection
[0m17:45:19.219564 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=4379848032, session-id=01ef9963-c07f-13f1-80dd-a96a3504b15e, name=model.business_accounts.integrated_business_accounts, idle-time=3.0994415283203125e-06s, acquire-count=0, language=sql, thread-identifier=(23721, 6243577856), compute-name=) - Released connection
[0m17:45:19.220331 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7867f181-226b-4757-9d38-12bb29aa2b35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10217aa30>]}
[0m17:45:19.221219 [info ] [Thread-3  ]: 2 of 3 OK created sql view model default.integrated_business_accounts .......... [[32mOK[0m in 1.31s]
[0m17:45:19.222017 [debug] [Thread-3  ]: Finished running node model.business_accounts.integrated_business_accounts
[0m17:45:19.223048 [debug] [Thread-2  ]: Began running node model.business_accounts.prepared_business_accounts
[0m17:45:19.223718 [info ] [Thread-2  ]: 3 of 3 START sql table model default.prepared_business_accounts ................ [RUN]
[0m17:45:19.224498 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5091087696, session-id=None, name=model.business_accounts.prepared_business_accounts, idle-time=0s, acquire-count=0, language=None, thread-identifier=(23721, 6226751488), compute-name=) - Creating connection
[0m17:45:19.224965 [debug] [Thread-2  ]: Acquiring new databricks connection 'model.business_accounts.prepared_business_accounts'
[0m17:45:19.225402 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5091087696, session-id=None, name=model.business_accounts.prepared_business_accounts, idle-time=5.0067901611328125e-06s, acquire-count=1, language=sql, thread-identifier=(23721, 6226751488), compute-name=) - Acquired connection on thread (23721, 6226751488), using default compute resource for model '`hive_metastore`.`default`.`prepared_business_accounts`'
[0m17:45:19.225836 [debug] [Thread-2  ]: Began compiling node model.business_accounts.prepared_business_accounts
[0m17:45:19.232412 [debug] [Thread-2  ]: Writing injected SQL for node "model.business_accounts.prepared_business_accounts"
[0m17:45:19.234251 [debug] [Thread-2  ]: Began executing node model.business_accounts.prepared_business_accounts
[0m17:45:19.245055 [debug] [Thread-2  ]: MATERIALIZING TABLE
[0m17:45:19.250045 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5091087696, session-id=None, name=model.business_accounts.prepared_business_accounts, idle-time=0.0246579647064209s, acquire-count=1, language=sql, thread-identifier=(23721, 6226751488), compute-name=) - Checking idleness
[0m17:45:19.250436 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5091087696, session-id=None, name=model.business_accounts.prepared_business_accounts, idle-time=0.025091886520385742s, acquire-count=1, language=sql, thread-identifier=(23721, 6226751488), compute-name=) - Retrieving connection
[0m17:45:19.250737 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5091087696, session-id=None, name=model.business_accounts.prepared_business_accounts, idle-time=0.02539801597595215s, acquire-count=1, language=sql, thread-identifier=(23721, 6226751488), compute-name=) - Checking idleness
[0m17:45:19.251034 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5091087696, session-id=None, name=model.business_accounts.prepared_business_accounts, idle-time=0.025699138641357422s, acquire-count=1, language=sql, thread-identifier=(23721, 6226751488), compute-name=) - Retrieving connection
[0m17:45:19.251312 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:45:19.251518 [debug] [Thread-2  ]: Using databricks connection "model.business_accounts.prepared_business_accounts"
[0m17:45:19.251830 [debug] [Thread-2  ]: On model.business_accounts.prepared_business_accounts: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.prepared_business_accounts"} */

      describe extended `hive_metastore`.`default`.`prepared_business_accounts`
  
[0m17:45:19.252091 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m17:45:19.588770 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5091087696, session-id=01ef9963-c148-1f79-8797-538b247e96f0, name=model.business_accounts.prepared_business_accounts, idle-time=2.7179718017578125e-05s, acquire-count=1, language=sql, thread-identifier=(23721, 6226751488), compute-name=) - Connection created
[0m17:45:19.590813 [debug] [Thread-2  ]: Databricks adapter: Cursor(session-id=01ef9963-c148-1f79-8797-538b247e96f0, command-id=Unknown) - Created cursor
[0m17:45:20.238343 [debug] [Thread-2  ]: SQL status: OK in 0.990 seconds
[0m17:45:20.245275 [debug] [Thread-2  ]: Databricks adapter: Cursor(session-id=01ef9963-c148-1f79-8797-538b247e96f0, command-id=01ef9963-c15b-11ae-8f55-c32f5134b54f) - Closing cursor
[0m17:45:20.275721 [debug] [Thread-2  ]: Writing runtime sql for node "model.business_accounts.prepared_business_accounts"
[0m17:45:20.276565 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5091087696, session-id=01ef9963-c148-1f79-8797-538b247e96f0, name=model.business_accounts.prepared_business_accounts, idle-time=0.6885111331939697s, acquire-count=1, language=sql, thread-identifier=(23721, 6226751488), compute-name=) - Checking idleness
[0m17:45:20.276890 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5091087696, session-id=01ef9963-c148-1f79-8797-538b247e96f0, name=model.business_accounts.prepared_business_accounts, idle-time=0.68886399269104s, acquire-count=1, language=sql, thread-identifier=(23721, 6226751488), compute-name=) - Retrieving connection
[0m17:45:20.277105 [debug] [Thread-2  ]: Using databricks connection "model.business_accounts.prepared_business_accounts"
[0m17:45:20.277448 [debug] [Thread-2  ]: On model.business_accounts.prepared_business_accounts: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.prepared_business_accounts"} */

  
    
        create or replace table `hive_metastore`.`default`.`prepared_business_accounts`
      
      using delta
      
      
      
      
      
      
      
      as
      

with transaction_metrics as (
    select
        account_id,
        count(*) as total_number_of_transactions,
        
    sum(transaction_amount)
 as total_transaction_volume,
        min(transaction_date) as first_transaction_date,
        max(transaction_date) as last_transaction_date
    from `hive_metastore`.`raw`.`raw_transactions`
    group by account_id
),
business_accounts_prepared as (
    select
        a.account_id,
        a.account_name,
        a.contact_email,
        a.registration_date,
        t.total_number_of_transactions,
        t.total_transaction_volume,
        t.first_transaction_date,
        t.last_transaction_date
    from `hive_metastore`.`default`.`integrated_business_accounts` a
    left join transaction_metrics t
        on a.account_id = t.account_id
)

select
    account_id,
    account_name,
    contact_email,
    registration_date,
    total_number_of_transactions,
    total_transaction_volume,
    first_transaction_date,
    last_transaction_date,
    
    CAST(current_timestamp() AS TIMESTAMP) as created_at,
    CAST(current_timestamp() AS TIMESTAMP) as updated_at,
    CAST(current_timestamp() AS TIMESTAMP) as processed_at

from business_accounts_prepared;
  
[0m17:45:20.277768 [debug] [Thread-2  ]: Databricks adapter: Cursor(session-id=01ef9963-c148-1f79-8797-538b247e96f0, command-id=Unknown) - Created cursor
[0m17:45:27.302073 [debug] [Thread-2  ]: SQL status: OK in 7.020 seconds
[0m17:45:27.304297 [debug] [Thread-2  ]: Databricks adapter: Cursor(session-id=01ef9963-c148-1f79-8797-538b247e96f0, command-id=01ef9963-c1c3-17d9-bec9-eae15ba6bd60) - Closing cursor
[0m17:45:27.478033 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5091087696, session-id=01ef9963-c148-1f79-8797-538b247e96f0, name=model.business_accounts.prepared_business_accounts, idle-time=4.0531158447265625e-06s, acquire-count=0, language=sql, thread-identifier=(23721, 6226751488), compute-name=) - Released connection
[0m17:45:27.478748 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5091087696, session-id=01ef9963-c148-1f79-8797-538b247e96f0, name=model.business_accounts.prepared_business_accounts, idle-time=1.6689300537109375e-06s, acquire-count=0, language=sql, thread-identifier=(23721, 6226751488), compute-name=) - Released connection
[0m17:45:27.479249 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7867f181-226b-4757-9d38-12bb29aa2b35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x146a974c0>]}
[0m17:45:27.479946 [info ] [Thread-2  ]: 3 of 3 OK created sql table model default.prepared_business_accounts ........... [[32mOK[0m in 8.25s]
[0m17:45:27.480542 [debug] [Thread-2  ]: Finished running node model.business_accounts.prepared_business_accounts
[0m17:45:27.481819 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5480171840, session-id=None, name=master, idle-time=10.343682050704956s, acquire-count=0, language=None, thread-identifier=(23721, 8635354944), compute-name=) - Checking idleness
[0m17:45:27.482259 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5480171840, session-id=None, name=master, idle-time=10.34415316581726s, acquire-count=0, language=None, thread-identifier=(23721, 8635354944), compute-name=) - Reusing connection previously named master
[0m17:45:27.482589 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5480171840, session-id=None, name=master, idle-time=10.344485998153687s, acquire-count=1, language=None, thread-identifier=(23721, 8635354944), compute-name=) - Acquired connection on thread (23721, 8635354944), using default compute resource
[0m17:45:27.482882 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5480171840, session-id=None, name=master, idle-time=10.344794988632202s, acquire-count=1, language=None, thread-identifier=(23721, 8635354944), compute-name=) - Checking idleness
[0m17:45:27.483145 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5480171840, session-id=None, name=master, idle-time=10.345056056976318s, acquire-count=1, language=None, thread-identifier=(23721, 8635354944), compute-name=) - Retrieving connection
[0m17:45:27.483390 [debug] [MainThread]: On master: ROLLBACK
[0m17:45:27.483627 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:45:27.803385 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5480171840, session-id=01ef9963-c631-109a-8dfb-d4ec4ec1bac8, name=master, idle-time=1.3113021850585938e-05s, acquire-count=1, language=None, thread-identifier=(23721, 8635354944), compute-name=) - Connection created
[0m17:45:27.804397 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:45:27.805027 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5480171840, session-id=01ef9963-c631-109a-8dfb-d4ec4ec1bac8, name=master, idle-time=0.0018711090087890625s, acquire-count=1, language=None, thread-identifier=(23721, 8635354944), compute-name=) - Checking idleness
[0m17:45:27.805566 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5480171840, session-id=01ef9963-c631-109a-8dfb-d4ec4ec1bac8, name=master, idle-time=0.0024230480194091797s, acquire-count=1, language=None, thread-identifier=(23721, 8635354944), compute-name=) - Retrieving connection
[0m17:45:27.806052 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:45:27.806497 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:45:27.807028 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5480171840, session-id=01ef9963-c631-109a-8dfb-d4ec4ec1bac8, name=master, idle-time=3.0994415283203125e-06s, acquire-count=0, language=None, thread-identifier=(23721, 8635354944), compute-name=) - Released connection
[0m17:45:27.807782 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:45:27.808253 [debug] [MainThread]: On master: ROLLBACK
[0m17:45:27.808708 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:45:27.809095 [debug] [MainThread]: On master: Close
[0m17:45:27.809497 [debug] [MainThread]: Databricks adapter: Connection(session-id=01ef9963-c631-109a-8dfb-d4ec4ec1bac8) - Closing connection
[0m17:45:27.915577 [debug] [MainThread]: Connection 'model.business_accounts.staging_business_accounts' was properly closed.
[0m17:45:27.915910 [debug] [MainThread]: On model.business_accounts.staging_business_accounts: ROLLBACK
[0m17:45:27.916119 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:45:27.916317 [debug] [MainThread]: On model.business_accounts.staging_business_accounts: Close
[0m17:45:27.916533 [debug] [MainThread]: Databricks adapter: Connection(session-id=01ef9963-bf42-1dbf-a576-09ad6cb8faf3) - Closing connection
[0m17:45:28.008491 [debug] [MainThread]: Connection 'model.business_accounts.integrated_business_accounts' was properly closed.
[0m17:45:28.009058 [debug] [MainThread]: On model.business_accounts.integrated_business_accounts: ROLLBACK
[0m17:45:28.009545 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:45:28.010009 [debug] [MainThread]: On model.business_accounts.integrated_business_accounts: Close
[0m17:45:28.010525 [debug] [MainThread]: Databricks adapter: Connection(session-id=01ef9963-c07f-13f1-80dd-a96a3504b15e) - Closing connection
[0m17:45:28.110369 [debug] [MainThread]: Connection 'model.business_accounts.prepared_business_accounts' was properly closed.
[0m17:45:28.111963 [debug] [MainThread]: On model.business_accounts.prepared_business_accounts: ROLLBACK
[0m17:45:28.112827 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:45:28.113519 [debug] [MainThread]: On model.business_accounts.prepared_business_accounts: Close
[0m17:45:28.114196 [debug] [MainThread]: Databricks adapter: Connection(session-id=01ef9963-c148-1f79-8797-538b247e96f0) - Closing connection
[0m17:45:28.211913 [info ] [MainThread]: 
[0m17:45:28.212728 [info ] [MainThread]: Finished running 2 view models, 1 table model in 0 hours 0 minutes and 12.36 seconds (12.36s).
[0m17:45:28.214115 [debug] [MainThread]: Command end result
[0m17:45:28.251964 [info ] [MainThread]: 
[0m17:45:28.252351 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:45:28.252581 [info ] [MainThread]: 
[0m17:45:28.252814 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m17:45:28.254873 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 13.581927, "process_user_time": 2.386829, "process_kernel_time": 2.318434, "process_mem_max_rss": "226869248", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:45:28.255240 [debug] [MainThread]: Command `dbt run` succeeded at 17:45:28.255178 after 13.58 seconds
[0m17:45:28.255513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102b23f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ffc0f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f707190>]}
[0m17:45:28.255773 [debug] [MainThread]: Flushing usage events
[0m15:12:57.504039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105238ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081c3520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081c3160>]}


============================== 15:12:57.507763 | da8ecab6-4a33-440f-82b9-054140c88055 ==============================
[0m15:12:57.507763 [info ] [MainThread]: Running with dbt=1.8.7
[0m15:12:57.508171 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/ankurchopra/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:12:57.563100 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:12:57.563501 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:12:57.563687 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:12:58.407920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'da8ecab6-4a33-440f-82b9-054140c88055', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15279d070>]}
[0m15:12:58.439453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'da8ecab6-4a33-440f-82b9-054140c88055', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x153566760>]}
[0m15:12:58.440006 [info ] [MainThread]: Registered adapter: databricks=1.8.7
[0m15:12:58.457647 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m15:12:58.554197 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 4 files changed.
[0m15:12:58.554763 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/staging/staging_business_accounts.sql
[0m15:12:58.555003 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/integrated/integrated_business_accounts.sql
[0m15:12:58.555256 [debug] [MainThread]: Partial parsing: updated file: business_accounts://macros/macros.sql
[0m15:12:58.555443 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/prepared/prepared_business_accounts.sql
[0m15:12:58.662584 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.business_accounts.staging_business_accounts' (models/business_accounts/staging/staging_business_accounts.sql) depends on a node named 'raw.raw_business_accounts' which was not found
[0m15:12:58.713202 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.25554, "process_user_time": 1.819147, "process_kernel_time": 2.307118, "process_mem_max_rss": "209715200", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m15:12:58.713659 [debug] [MainThread]: Command `dbt run` failed at 15:12:58.713594 after 1.26 seconds
[0m15:12:58.713943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105238ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1537e83d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10567f490>]}
[0m15:12:58.714203 [debug] [MainThread]: Flushing usage events
[0m15:17:37.853784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102738eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e034f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e03100>]}


============================== 15:17:37.856728 | 8bac4976-0a38-4cb5-9097-d5dabb3735ee ==============================
[0m15:17:37.856728 [info ] [MainThread]: Running with dbt=1.8.7
[0m15:17:37.857091 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/ankurchopra/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:17:37.897738 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:17:37.898078 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:17:37.898261 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:17:38.616793 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8bac4976-0a38-4cb5-9097-d5dabb3735ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1052371c0>]}
[0m15:17:38.644235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8bac4976-0a38-4cb5-9097-d5dabb3735ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104dce9d0>]}
[0m15:17:38.644604 [info ] [MainThread]: Registered adapter: databricks=1.8.7
[0m15:17:38.662769 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m15:17:38.717046 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m15:17:38.717468 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8bac4976-0a38-4cb5-9097-d5dabb3735ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1450fce80>]}
[0m15:17:39.635953 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.business_accounts.staging_business_accounts' (models/business_accounts/staging/staging_business_accounts.sql) depends on a node named 'raw.raw_business_accounts' which was not found
[0m15:17:39.638130 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.8215264, "process_user_time": 2.555053, "process_kernel_time": 2.294437, "process_mem_max_rss": "210436096", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m15:17:39.638435 [debug] [MainThread]: Command `dbt run` failed at 15:17:39.638380 after 1.82 seconds
[0m15:17:39.638654 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102738eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x145217790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1452fc1c0>]}
[0m15:17:39.638854 [debug] [MainThread]: Flushing usage events
[0m15:18:36.100950 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102ba7f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104646550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104646130>]}


============================== 15:18:36.103954 | 834f2512-b071-4033-a86b-23acd6380733 ==============================
[0m15:18:36.103954 [info ] [MainThread]: Running with dbt=1.8.7
[0m15:18:36.104315 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/Users/ankurchopra/.dbt', 'log_path': '/Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:18:36.144761 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:18:36.145149 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:18:36.145341 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:19:37.423086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c57f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071fe550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071fe130>]}


============================== 15:19:37.426399 | a5f1c3eb-265d-44bc-a65b-675f421785d0 ==============================
[0m15:19:37.426399 [info ] [MainThread]: Running with dbt=1.8.7
[0m15:19:37.426793 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/ankurchopra/.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m15:19:37.466645 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:19:37.467009 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:19:37.467191 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:19:38.181051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a5f1c3eb-265d-44bc-a65b-675f421785d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071cac70>]}
[0m15:19:38.208264 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a5f1c3eb-265d-44bc-a65b-675f421785d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071ca9d0>]}
[0m15:19:38.208640 [info ] [MainThread]: Registered adapter: databricks=1.8.7
[0m15:19:38.223913 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m15:19:38.311110 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 5 files changed.
[0m15:19:38.311699 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/sources.yml
[0m15:19:38.311911 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/integrated/integrated_business_accounts.sql
[0m15:19:38.312097 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/prepared/prepared_business_accounts.sql
[0m15:19:38.312275 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/staging/staging_business_accounts.sql
[0m15:19:38.312486 [debug] [MainThread]: Partial parsing: updated file: business_accounts://macros/macros.sql
[0m15:19:38.455647 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.business_accounts.prepared_business_accounts' (models/business_accounts/prepared/prepared_business_accounts.sql) depends on a node named 'integrated.int_business_accounts' which was not found
[0m15:19:38.458091 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.0787163, "process_user_time": 1.786923, "process_kernel_time": 2.323362, "process_mem_max_rss": "209584128", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m15:19:38.458440 [debug] [MainThread]: Command `dbt run` failed at 15:19:38.458381 after 1.08 seconds
[0m15:19:38.458685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103c57f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10511a400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15209deb0>]}
[0m15:19:38.458919 [debug] [MainThread]: Flushing usage events
[0m15:20:10.747763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102f07f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049a65b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049a6100>]}


============================== 15:20:10.750546 | dd77d475-1c46-41eb-9721-b3005de4f4d2 ==============================
[0m15:20:10.750546 [info ] [MainThread]: Running with dbt=1.8.7
[0m15:20:10.750885 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/logs', 'debug': 'False', 'profiles_dir': '/Users/ankurchopra/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:20:10.790939 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:20:10.791298 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:20:10.791487 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:20:11.531597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dd77d475-1c46-41eb-9721-b3005de4f4d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122518610>]}
[0m15:20:11.559042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dd77d475-1c46-41eb-9721-b3005de4f4d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10494af40>]}
[0m15:20:11.559411 [info ] [MainThread]: Registered adapter: databricks=1.8.7
[0m15:20:11.574359 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m15:20:11.656364 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 5 files changed.
[0m15:20:11.656960 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/sources.yml
[0m15:20:11.657167 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/staging/staging_business_accounts.sql
[0m15:20:11.657351 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/integrated/integrated_business_accounts.sql
[0m15:20:11.657556 [debug] [MainThread]: Partial parsing: updated file: business_accounts://macros/macros.sql
[0m15:20:11.657732 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/prepared/prepared_business_accounts.sql
[0m15:20:11.800753 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.business_accounts.prepared_business_accounts' (models/business_accounts/prepared/prepared_business_accounts.sql) depends on a node named 'integrated.integrated_business_accounts' which was not found
[0m15:20:11.803544 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.0938821, "process_user_time": 1.796408, "process_kernel_time": 2.288258, "process_mem_max_rss": "211533824", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m15:20:11.803870 [debug] [MainThread]: Command `dbt run` failed at 15:20:11.803813 after 1.09 seconds
[0m15:20:11.804113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102f07f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1230de0a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122fdab80>]}
[0m15:20:11.804330 [debug] [MainThread]: Flushing usage events
[0m15:33:52.486329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071d4ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c73520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c73160>]}


============================== 15:33:52.489326 | 94969885-c63f-4fa8-a931-1dfc80194ec6 ==============================
[0m15:33:52.489326 [info ] [MainThread]: Running with dbt=1.8.7
[0m15:33:52.489687 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/logs', 'version_check': 'True', 'profiles_dir': '/Users/ankurchopra/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m15:33:52.530763 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:33:52.531135 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:33:52.531322 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:33:53.296632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '94969885-c63f-4fa8-a931-1dfc80194ec6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126fc2070>]}
[0m15:33:53.324615 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '94969885-c63f-4fa8-a931-1dfc80194ec6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12757f760>]}
[0m15:33:53.325025 [info ] [MainThread]: Registered adapter: databricks=1.8.7
[0m15:33:53.340508 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m15:33:53.426750 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m15:33:53.427199 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/staging/staging_business_accounts.sql
[0m15:33:53.427444 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/integrated/integrated_business_accounts.sql
[0m15:33:53.427667 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/prepared/prepared_business_accounts.sql
[0m15:33:53.526083 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.business_accounts.staging_business_accounts' (models/business_accounts/staging/staging_business_accounts.sql) depends on a node named 'raw.raw_business_accounts' which was not found
[0m15:33:53.529140 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.0805295, "process_user_time": 1.729103, "process_kernel_time": 2.333055, "process_mem_max_rss": "210124800", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m15:33:53.529554 [debug] [MainThread]: Command `dbt run` failed at 15:33:53.529486 after 1.08 seconds
[0m15:33:53.529819 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071d4ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x132ad6940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1274b12e0>]}
[0m15:33:53.530066 [debug] [MainThread]: Flushing usage events
[0m15:42:08.038797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10298ff40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10442f5b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10442f100>]}


============================== 15:42:08.041746 | a8c0b660-7b39-417a-bbf6-0948385458ee ==============================
[0m15:42:08.041746 [info ] [MainThread]: Running with dbt=1.8.7
[0m15:42:08.042097 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/logs', 'version_check': 'True', 'profiles_dir': '/Users/ankurchopra/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:42:08.083019 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:42:08.083360 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:42:08.083561 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:42:08.836644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a8c0b660-7b39-417a-bbf6-0948385458ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a3d8610>]}
[0m15:42:08.863961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a8c0b660-7b39-417a-bbf6-0948385458ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043d2f40>]}
[0m15:42:08.864319 [info ] [MainThread]: Registered adapter: databricks=1.8.7
[0m15:42:08.879134 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m15:42:08.961528 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 4 files changed.
[0m15:42:08.961981 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/staging/staging_business_accounts.sql
[0m15:42:08.962218 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/prepared/prepared_business_accounts.sql
[0m15:42:08.962438 [debug] [MainThread]: Partial parsing: updated file: business_accounts://macros/macros.sql
[0m15:42:08.962656 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/integrated/integrated_business_accounts.sql
[0m15:42:09.059765 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.business_accounts.prepared_business_accounts' (models/business_accounts/prepared/prepared_business_accounts.sql) depends on a node named 'integrated.integrated_business_accounts' which was not found
[0m15:42:09.062659 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.0606217, "process_user_time": 1.74954, "process_kernel_time": 2.325002, "process_mem_max_rss": "209207296", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m15:42:09.063054 [debug] [MainThread]: Command `dbt run` failed at 15:42:09.062989 after 1.06 seconds
[0m15:42:09.063336 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10298ff40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12bba5eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12bba5f40>]}
[0m15:42:09.063611 [debug] [MainThread]: Flushing usage events
[0m15:44:12.814336 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1052bbf40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10787f5b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10787f100>]}


============================== 15:44:12.817332 | 013b42d2-48b4-4632-b219-71f64982df0b ==============================
[0m15:44:12.817332 [info ] [MainThread]: Running with dbt=1.8.7
[0m15:44:12.817682 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/ankurchopra/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:44:12.859287 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:44:12.859679 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:44:12.859882 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:44:13.601839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '013b42d2-48b4-4632-b219-71f64982df0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x128b18610>]}
[0m15:44:13.629387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '013b42d2-48b4-4632-b219-71f64982df0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107822f40>]}
[0m15:44:13.629743 [info ] [MainThread]: Registered adapter: databricks=1.8.7
[0m15:44:13.644899 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m15:44:13.727598 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 4 files changed.
[0m15:44:13.728089 [debug] [MainThread]: Partial parsing: updated file: business_accounts://macros/macros.sql
[0m15:44:13.728330 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/staging/staging_business_accounts.sql
[0m15:44:13.728516 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/prepared/prepared_business_accounts.sql
[0m15:44:13.728719 [debug] [MainThread]: Partial parsing: updated file: business_accounts://models/business_accounts/integrated/integrated_business_accounts.sql
[0m15:44:13.824424 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.business_accounts.prepared_business_accounts' (models/business_accounts/prepared/prepared_business_accounts.sql) depends on a node named 'integrated.integrated_business_accounts' which was not found
[0m15:44:13.827295 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.0519332, "process_user_time": 1.74048, "process_kernel_time": 2.310679, "process_mem_max_rss": "208142336", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m15:44:13.827623 [debug] [MainThread]: Command `dbt run` failed at 15:44:13.827565 after 1.05 seconds
[0m15:44:13.827877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1052bbf40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129774700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c4e970>]}
[0m15:44:13.828129 [debug] [MainThread]: Flushing usage events
[0m15:46:50.261915 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106dd4ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10917f520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10917f160>]}


============================== 15:46:50.264983 | adf13488-b9f7-4f80-89dd-0b42be4f4289 ==============================
[0m15:46:50.264983 [info ] [MainThread]: Running with dbt=1.8.7
[0m15:46:50.265331 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/Users/ankurchopra/.dbt', 'log_path': '/Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:46:50.306928 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:46:50.307288 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:46:50.307484 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:46:51.061904 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'adf13488-b9f7-4f80-89dd-0b42be4f4289', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14a2f2070>]}
[0m15:46:51.089335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'adf13488-b9f7-4f80-89dd-0b42be4f4289', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14b0bb760>]}
[0m15:46:51.089700 [info ] [MainThread]: Registered adapter: databricks=1.8.7
[0m15:46:51.104269 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m15:46:51.159242 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m15:46:51.159614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'adf13488-b9f7-4f80-89dd-0b42be4f4289', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14a2cb1c0>]}
[0m15:46:52.085314 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.business_accounts.prepared_business_accounts' (models/business_accounts/prepared/prepared_business_accounts.sql) depends on a node named 'integrated.integrated_business_accounts' which was not found
[0m15:46:52.087836 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.8693255, "process_user_time": 2.554234, "process_kernel_time": 2.316237, "process_mem_max_rss": "210354176", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m15:46:52.088126 [debug] [MainThread]: Command `dbt run` failed at 15:46:52.088076 after 1.87 seconds
[0m15:46:52.088347 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106dd4ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14b36d430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14b0137c0>]}
[0m15:46:52.088562 [debug] [MainThread]: Flushing usage events
[0m15:47:06.308678 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c87f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110e43550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110e43100>]}


============================== 15:47:06.311357 | 92fb6c90-1631-4968-b94e-3d80145ec4aa ==============================
[0m15:47:06.311357 [info ] [MainThread]: Running with dbt=1.8.7
[0m15:47:06.311692 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/ankurchopra/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:47:06.349124 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:47:06.349473 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:47:06.349669 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:47:07.053330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '92fb6c90-1631-4968-b94e-3d80145ec4aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110eff7f0>]}
[0m15:47:07.080529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '92fb6c90-1631-4968-b94e-3d80145ec4aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15027b760>]}
[0m15:47:07.080886 [info ] [MainThread]: Registered adapter: databricks=1.8.7
[0m15:47:07.095990 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m15:47:07.149213 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m15:47:07.149591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '92fb6c90-1631-4968-b94e-3d80145ec4aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10795f640>]}
[0m15:47:08.069205 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.business_accounts.integrated_business_accounts' (models/business_accounts/integrated/integrated_business_accounts.sql) depends on a node named 'staging.staging_business_accounts' which was not found
[0m15:47:08.071247 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.7992761, "process_user_time": 2.53214, "process_kernel_time": 2.289573, "process_mem_max_rss": "209977344", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m15:47:08.071552 [debug] [MainThread]: Command `dbt run` failed at 15:47:08.071499 after 1.80 seconds
[0m15:47:08.071779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c87f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x150190880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15027bc10>]}
[0m15:47:08.071995 [debug] [MainThread]: Flushing usage events
[0m15:47:21.286573 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103323f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104dc2550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104dc2130>]}


============================== 15:47:21.289673 | 7197c6e1-08b7-4bdd-acfc-a3ab44159fe1 ==============================
[0m15:47:21.289673 [info ] [MainThread]: Running with dbt=1.8.7
[0m15:47:21.290041 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/ankurchopra/repo_projects/ai-codecraft/smart_code_generator/generated_code/dbt_project/logs', 'version_check': 'True', 'profiles_dir': '/Users/ankurchopra/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:47:21.327406 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:47:21.327764 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:47:21.327958 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:47:22.030594 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7197c6e1-08b7-4bdd-acfc-a3ab44159fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d8ec70>]}
[0m15:47:22.057655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7197c6e1-08b7-4bdd-acfc-a3ab44159fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d8e9d0>]}
[0m15:47:22.058022 [info ] [MainThread]: Registered adapter: databricks=1.8.7
[0m15:47:22.073357 [debug] [MainThread]: checksum: 4af21dafb485259c48497ac86b711ddb1982f3d0f1c0ca4e09356de488b753c0, vars: {}, profile: , target: , version: 1.8.7
[0m15:47:22.127723 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m15:47:22.128111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '7197c6e1-08b7-4bdd-acfc-a3ab44159fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13da39160>]}
[0m15:47:23.092709 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 3 unused configuration paths:
- models.business_accounts.staging
- models.business_accounts.prepared
- models.business_accounts.integrated
[0m15:47:23.099865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7197c6e1-08b7-4bdd-acfc-a3ab44159fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13fb61f10>]}
[0m15:47:23.161542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7197c6e1-08b7-4bdd-acfc-a3ab44159fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13fb609d0>]}
[0m15:47:23.161886 [info ] [MainThread]: Found 3 models, 2 sources, 603 macros
[0m15:47:23.162082 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7197c6e1-08b7-4bdd-acfc-a3ab44159fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13fbef790>]}
[0m15:47:23.162899 [info ] [MainThread]: 
[0m15:47:23.163264 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5364503024, session-id=None, name=master, idle-time=0s, acquire-count=0, language=None, thread-identifier=(40701, 8635354944), compute-name=) - Creating connection
[0m15:47:23.163443 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:47:23.163615 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5364503024, session-id=None, name=master, idle-time=2.1457672119140625e-06s, acquire-count=1, language=None, thread-identifier=(40701, 8635354944), compute-name=) - Acquired connection on thread (40701, 8635354944), using default compute resource
[0m15:47:23.167311 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=None, name=list_hive_metastore, idle-time=0s, acquire-count=0, language=None, thread-identifier=(40701, 6194655232), compute-name=) - Creating connection
[0m15:47:23.167541 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m15:47:23.167723 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=None, name=list_hive_metastore, idle-time=9.5367431640625e-07s, acquire-count=1, language=None, thread-identifier=(40701, 6194655232), compute-name=) - Acquired connection on thread (40701, 6194655232), using default compute resource
[0m15:47:23.167909 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=None, name=list_hive_metastore, idle-time=0.00019288063049316406s, acquire-count=1, language=None, thread-identifier=(40701, 6194655232), compute-name=) - Checking idleness
[0m15:47:23.168072 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=None, name=list_hive_metastore, idle-time=0.0003578662872314453s, acquire-count=1, language=None, thread-identifier=(40701, 6194655232), compute-name=) - Retrieving connection
[0m15:47:23.168220 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m15:47:23.168374 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m15:47:23.168521 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:47:23.695402 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, name=list_hive_metastore, idle-time=9.298324584960938e-06s, acquire-count=1, language=None, thread-identifier=(40701, 6194655232), compute-name=) - Connection created
[0m15:47:23.695988 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, command-id=Unknown) - Created cursor
[0m15:47:32.509723 [debug] [ThreadPool]: SQL status: OK in 9.340 seconds
[0m15:47:32.519539 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, command-id=01ef9a24-d70c-1d74-a360-fa77232f8b2c) - Closing cursor
[0m15:47:32.520412 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, name=list_hive_metastore, idle-time=6.9141387939453125e-06s, acquire-count=0, language=None, thread-identifier=(40701, 6194655232), compute-name=) - Released connection
[0m15:47:32.522291 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, name=list_hive_metastore, idle-time=0.0019011497497558594s, acquire-count=0, language=None, thread-identifier=(40701, 6194655232), compute-name=) - Checking idleness
[0m15:47:32.522611 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m15:47:32.522867 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, name=list_hive_metastore_default, idle-time=0.0024929046630859375s, acquire-count=0, language=None, thread-identifier=(40701, 6194655232), compute-name=) - Reusing connection previously named list_hive_metastore
[0m15:47:32.523118 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, name=list_hive_metastore_default, idle-time=0.0027420520782470703s, acquire-count=1, language=None, thread-identifier=(40701, 6194655232), compute-name=) - Acquired connection on thread (40701, 6194655232), using default compute resource
[0m15:47:32.523375 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, name=list_hive_metastore_default, idle-time=0.0030100345611572266s, acquire-count=1, language=None, thread-identifier=(40701, 6194655232), compute-name=) - Checking idleness
[0m15:47:32.523616 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, name=list_hive_metastore_default, idle-time=0.003248929977416992s, acquire-count=1, language=None, thread-identifier=(40701, 6194655232), compute-name=) - Retrieving connection
[0m15:47:32.523827 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m15:47:32.524049 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m15:47:32.524290 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, command-id=Unknown) - Created cursor
[0m15:47:32.844978 [debug] [ThreadPool]: SQL status: OK in 0.320 seconds
[0m15:47:32.849992 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, command-id=01ef9a24-d93b-152a-8ffb-77cb130af195) - Closing cursor
[0m15:47:32.862757 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, name=list_hive_metastore_default, idle-time=0.34232401847839355s, acquire-count=1, language=None, thread-identifier=(40701, 6194655232), compute-name=) - Checking idleness
[0m15:47:32.863250 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, name=list_hive_metastore_default, idle-time=0.3428611755371094s, acquire-count=1, language=None, thread-identifier=(40701, 6194655232), compute-name=) - Retrieving connection
[0m15:47:32.863548 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, name=list_hive_metastore_default, idle-time=0.34317493438720703s, acquire-count=1, language=None, thread-identifier=(40701, 6194655232), compute-name=) - Checking idleness
[0m15:47:32.863822 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, name=list_hive_metastore_default, idle-time=0.34345102310180664s, acquire-count=1, language=None, thread-identifier=(40701, 6194655232), compute-name=) - Retrieving connection
[0m15:47:32.864071 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:47:32.864303 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m15:47:32.864562 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m15:47:32.864837 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, command-id=Unknown) - Created cursor
[0m15:47:33.592477 [debug] [ThreadPool]: SQL status: OK in 0.730 seconds
[0m15:47:33.596192 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, command-id=01ef9a24-d96f-1793-8c54-698c699d6684) - Closing cursor
[0m15:47:33.603755 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, name=list_hive_metastore_default, idle-time=1.0833239555358887s, acquire-count=1, language=None, thread-identifier=(40701, 6194655232), compute-name=) - Checking idleness
[0m15:47:33.604224 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, name=list_hive_metastore_default, idle-time=1.083834171295166s, acquire-count=1, language=None, thread-identifier=(40701, 6194655232), compute-name=) - Retrieving connection
[0m15:47:33.604518 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m15:47:33.604834 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m15:47:33.605159 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, command-id=Unknown) - Created cursor
[0m15:47:34.155314 [debug] [ThreadPool]: SQL status: OK in 0.550 seconds
[0m15:47:34.161227 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, command-id=01ef9a24-d9e0-190a-a097-2abcbe4dc177) - Closing cursor
[0m15:47:34.162323 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, name=list_hive_metastore_default, idle-time=5.0067901611328125e-06s, acquire-count=0, language=None, thread-identifier=(40701, 6194655232), compute-name=) - Released connection
[0m15:47:34.164420 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7197c6e1-08b7-4bdd-acfc-a3ab44159fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13da64280>]}
[0m15:47:34.165074 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5364503024, session-id=None, name=master, idle-time=11.001415014266968s, acquire-count=1, language=None, thread-identifier=(40701, 8635354944), compute-name=) - Checking idleness
[0m15:47:34.165467 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5364503024, session-id=None, name=master, idle-time=11.00182294845581s, acquire-count=1, language=None, thread-identifier=(40701, 8635354944), compute-name=) - Retrieving connection
[0m15:47:34.165802 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5364503024, session-id=None, name=master, idle-time=11.00217080116272s, acquire-count=1, language=None, thread-identifier=(40701, 8635354944), compute-name=) - Checking idleness
[0m15:47:34.166099 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5364503024, session-id=None, name=master, idle-time=11.002467155456543s, acquire-count=1, language=None, thread-identifier=(40701, 8635354944), compute-name=) - Retrieving connection
[0m15:47:34.166377 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:47:34.166644 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:47:34.166935 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5364503024, session-id=None, name=master, idle-time=2.1457672119140625e-06s, acquire-count=0, language=None, thread-identifier=(40701, 8635354944), compute-name=) - Released connection
[0m15:47:34.167380 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:47:34.167741 [info ] [MainThread]: 
[0m15:47:34.172690 [debug] [Thread-1  ]: Began running node model.business_accounts.staging_business_accounts
[0m15:47:34.173410 [info ] [Thread-1  ]: 1 of 3 START sql view model default.staging_business_accounts .................. [RUN]
[0m15:47:34.173992 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, name=list_hive_metastore_default, idle-time=0.011651992797851562s, acquire-count=0, language=None, thread-identifier=(40701, 6194655232), compute-name=) - Checking idleness
[0m15:47:34.174298 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.business_accounts.staging_business_accounts)
[0m15:47:34.174640 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, name=model.business_accounts.staging_business_accounts, idle-time=0.012321949005126953s, acquire-count=0, language=None, thread-identifier=(40701, 6194655232), compute-name=) - Reusing connection previously named list_hive_metastore_default
[0m15:47:34.174991 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, name=model.business_accounts.staging_business_accounts, idle-time=0.01266789436340332s, acquire-count=1, language=sql, thread-identifier=(40701, 6194655232), compute-name=) - Acquired connection on thread (40701, 6194655232), using default compute resource for model '`hive_metastore`.`default`.`staging_business_accounts`'
[0m15:47:34.175458 [debug] [Thread-1  ]: Began compiling node model.business_accounts.staging_business_accounts
[0m15:47:34.184025 [debug] [Thread-1  ]: Writing injected SQL for node "model.business_accounts.staging_business_accounts"
[0m15:47:34.184789 [debug] [Thread-1  ]: Began executing node model.business_accounts.staging_business_accounts
[0m15:47:34.205165 [debug] [Thread-1  ]: Writing runtime sql for node "model.business_accounts.staging_business_accounts"
[0m15:47:34.206117 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, name=model.business_accounts.staging_business_accounts, idle-time=0.043801069259643555s, acquire-count=1, language=sql, thread-identifier=(40701, 6194655232), compute-name=) - Checking idleness
[0m15:47:34.206395 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, name=model.business_accounts.staging_business_accounts, idle-time=0.04410386085510254s, acquire-count=1, language=sql, thread-identifier=(40701, 6194655232), compute-name=) - Retrieving connection
[0m15:47:34.206580 [debug] [Thread-1  ]: Using databricks connection "model.business_accounts.staging_business_accounts"
[0m15:47:34.206838 [debug] [Thread-1  ]: On model.business_accounts.staging_business_accounts: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.staging_business_accounts"} */
create or replace view `hive_metastore`.`default`.`staging_business_accounts`
  
  
  
  as
    

with business_accounts as (
    select
        account_id,
        business_name as account_name,
        contact_email,
        to_date(registration_date, 'YYYY-MM-DD') as registration_date
    from `hive_metastore`.`raw`.`raw_business_accounts`
    where contact_email is not null
)

select *
from business_accounts;

[0m15:47:34.207102 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, command-id=Unknown) - Created cursor
[0m15:47:35.873202 [debug] [Thread-1  ]: SQL status: OK in 1.670 seconds
[0m15:47:35.874053 [debug] [Thread-1  ]: Databricks adapter: Cursor(session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, command-id=01ef9a24-da3c-1bec-a89d-68035721c189) - Closing cursor
[0m15:47:35.888695 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, name=model.business_accounts.staging_business_accounts, idle-time=2.86102294921875e-06s, acquire-count=0, language=sql, thread-identifier=(40701, 6194655232), compute-name=) - Released connection
[0m15:47:35.889149 [debug] [Thread-1  ]: Databricks adapter: DatabricksDBTConnection(id=5364657552, session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83, name=model.business_accounts.staging_business_accounts, idle-time=7.152557373046875e-07s, acquire-count=0, language=sql, thread-identifier=(40701, 6194655232), compute-name=) - Released connection
[0m15:47:35.890032 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7197c6e1-08b7-4bdd-acfc-a3ab44159fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13fe53ee0>]}
[0m15:47:35.890427 [info ] [Thread-1  ]: 1 of 3 OK created sql view model default.staging_business_accounts ............. [[32mOK[0m in 1.72s]
[0m15:47:35.890774 [debug] [Thread-1  ]: Finished running node model.business_accounts.staging_business_accounts
[0m15:47:35.891279 [debug] [Thread-3  ]: Began running node model.business_accounts.integrated_business_accounts
[0m15:47:35.891565 [info ] [Thread-3  ]: 2 of 3 START sql view model default.integrated_business_accounts ............... [RUN]
[0m15:47:35.891899 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5364839904, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0s, acquire-count=0, language=None, thread-identifier=(40701, 6234615808), compute-name=) - Creating connection
[0m15:47:35.892111 [debug] [Thread-3  ]: Acquiring new databricks connection 'model.business_accounts.integrated_business_accounts'
[0m15:47:35.892319 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5364839904, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=1.9073486328125e-06s, acquire-count=1, language=sql, thread-identifier=(40701, 6234615808), compute-name=) - Acquired connection on thread (40701, 6234615808), using default compute resource for model '`hive_metastore`.`default`.`integrated_business_accounts`'
[0m15:47:35.892513 [debug] [Thread-3  ]: Began compiling node model.business_accounts.integrated_business_accounts
[0m15:47:35.895001 [debug] [Thread-3  ]: Writing injected SQL for node "model.business_accounts.integrated_business_accounts"
[0m15:47:35.895694 [debug] [Thread-3  ]: Began executing node model.business_accounts.integrated_business_accounts
[0m15:47:35.898405 [debug] [Thread-3  ]: Writing runtime sql for node "model.business_accounts.integrated_business_accounts"
[0m15:47:35.899103 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5364839904, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0.006755828857421875s, acquire-count=1, language=sql, thread-identifier=(40701, 6234615808), compute-name=) - Checking idleness
[0m15:47:35.899344 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5364839904, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0.007027864456176758s, acquire-count=1, language=sql, thread-identifier=(40701, 6234615808), compute-name=) - Retrieving connection
[0m15:47:35.899564 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5364839904, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0.0072476863861083984s, acquire-count=1, language=sql, thread-identifier=(40701, 6234615808), compute-name=) - Checking idleness
[0m15:47:35.899775 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5364839904, session-id=None, name=model.business_accounts.integrated_business_accounts, idle-time=0.0074596405029296875s, acquire-count=1, language=sql, thread-identifier=(40701, 6234615808), compute-name=) - Retrieving connection
[0m15:47:35.899978 [debug] [Thread-3  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:47:35.900134 [debug] [Thread-3  ]: Using databricks connection "model.business_accounts.integrated_business_accounts"
[0m15:47:35.900389 [debug] [Thread-3  ]: On model.business_accounts.integrated_business_accounts: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.integrated_business_accounts"} */
create or replace view `hive_metastore`.`default`.`integrated_business_accounts`
  
  
  
  as
    

with deduplicated_accounts as (
    select
        account_id,
        max(account_name) as account_name,
        max(contact_email) as contact_email,
        max(registration_date) as registration_date
    from (
        select
            account_id,
            account_name,
            contact_email,
            registration_date
        from `hive_metastore`.`default`.`staging_business_accounts`
    ) as accounts
    group by account_id
),

transactions_sum as (
    select
        account_id,
        sum(transaction_amount) as total_transactions_amount
    from `hive_metastore`.`raw`.`raw_transactions`
    group by account_id
)

select
    a.account_id,
    a.account_name,
    a.contact_email,
    a.registration_date,
    coalesce(t.total_transactions_amount, 0) as total_transactions_amount
from deduplicated_accounts a
left join transactions_sum t
    on a.account_id = t.account_id
order by a.registration_date desc;

[0m15:47:35.900708 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m15:47:36.263510 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5364839904, session-id=01ef9a24-db66-16f1-b1ff-e4657fd57a32, name=model.business_accounts.integrated_business_accounts, idle-time=4.76837158203125e-06s, acquire-count=1, language=sql, thread-identifier=(40701, 6234615808), compute-name=) - Connection created
[0m15:47:36.264220 [debug] [Thread-3  ]: Databricks adapter: Cursor(session-id=01ef9a24-db66-16f1-b1ff-e4657fd57a32, command-id=Unknown) - Created cursor
[0m15:47:37.328044 [debug] [Thread-3  ]: SQL status: OK in 1.430 seconds
[0m15:47:37.330167 [debug] [Thread-3  ]: Databricks adapter: Cursor(session-id=01ef9a24-db66-16f1-b1ff-e4657fd57a32, command-id=01ef9a24-db76-106e-9eb9-1e318b8a2850) - Closing cursor
[0m15:47:37.332107 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5364839904, session-id=01ef9a24-db66-16f1-b1ff-e4657fd57a32, name=model.business_accounts.integrated_business_accounts, idle-time=7.867813110351562e-06s, acquire-count=0, language=sql, thread-identifier=(40701, 6234615808), compute-name=) - Released connection
[0m15:47:37.332841 [debug] [Thread-3  ]: Databricks adapter: DatabricksDBTConnection(id=5364839904, session-id=01ef9a24-db66-16f1-b1ff-e4657fd57a32, name=model.business_accounts.integrated_business_accounts, idle-time=2.1457672119140625e-06s, acquire-count=0, language=sql, thread-identifier=(40701, 6234615808), compute-name=) - Released connection
[0m15:47:37.333427 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7197c6e1-08b7-4bdd-acfc-a3ab44159fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13fc4f400>]}
[0m15:47:37.334324 [info ] [Thread-3  ]: 2 of 3 OK created sql view model default.integrated_business_accounts .......... [[32mOK[0m in 1.44s]
[0m15:47:37.335105 [debug] [Thread-3  ]: Finished running node model.business_accounts.integrated_business_accounts
[0m15:47:37.336149 [debug] [Thread-2  ]: Began running node model.business_accounts.prepared_business_accounts
[0m15:47:37.336688 [info ] [Thread-2  ]: 3 of 3 START sql table model default.prepared_business_accounts ................ [RUN]
[0m15:47:37.337330 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5364839424, session-id=None, name=model.business_accounts.prepared_business_accounts, idle-time=0s, acquire-count=0, language=None, thread-identifier=(40701, 6217789440), compute-name=) - Creating connection
[0m15:47:37.337741 [debug] [Thread-2  ]: Acquiring new databricks connection 'model.business_accounts.prepared_business_accounts'
[0m15:47:37.338154 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5364839424, session-id=None, name=model.business_accounts.prepared_business_accounts, idle-time=4.0531158447265625e-06s, acquire-count=1, language=sql, thread-identifier=(40701, 6217789440), compute-name=) - Acquired connection on thread (40701, 6217789440), using default compute resource for model '`hive_metastore`.`default`.`prepared_business_accounts`'
[0m15:47:37.338583 [debug] [Thread-2  ]: Began compiling node model.business_accounts.prepared_business_accounts
[0m15:47:37.344452 [debug] [Thread-2  ]: Writing injected SQL for node "model.business_accounts.prepared_business_accounts"
[0m15:47:37.346570 [debug] [Thread-2  ]: Began executing node model.business_accounts.prepared_business_accounts
[0m15:47:37.358471 [debug] [Thread-2  ]: MATERIALIZING TABLE
[0m15:47:37.363173 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5364839424, session-id=None, name=model.business_accounts.prepared_business_accounts, idle-time=0.02504110336303711s, acquire-count=1, language=sql, thread-identifier=(40701, 6217789440), compute-name=) - Checking idleness
[0m15:47:37.363551 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5364839424, session-id=None, name=model.business_accounts.prepared_business_accounts, idle-time=0.02544879913330078s, acquire-count=1, language=sql, thread-identifier=(40701, 6217789440), compute-name=) - Retrieving connection
[0m15:47:37.363866 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5364839424, session-id=None, name=model.business_accounts.prepared_business_accounts, idle-time=0.025769948959350586s, acquire-count=1, language=sql, thread-identifier=(40701, 6217789440), compute-name=) - Checking idleness
[0m15:47:37.364154 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5364839424, session-id=None, name=model.business_accounts.prepared_business_accounts, idle-time=0.026064157485961914s, acquire-count=1, language=sql, thread-identifier=(40701, 6217789440), compute-name=) - Retrieving connection
[0m15:47:37.364419 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:47:37.364621 [debug] [Thread-2  ]: Using databricks connection "model.business_accounts.prepared_business_accounts"
[0m15:47:37.364897 [debug] [Thread-2  ]: On model.business_accounts.prepared_business_accounts: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.prepared_business_accounts"} */

      describe extended `hive_metastore`.`default`.`prepared_business_accounts`
  
[0m15:47:37.365139 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m15:47:37.692330 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5364839424, session-id=01ef9a24-dc3e-1c09-9484-ccf75e837b1c, name=model.business_accounts.prepared_business_accounts, idle-time=2.7894973754882812e-05s, acquire-count=1, language=sql, thread-identifier=(40701, 6217789440), compute-name=) - Connection created
[0m15:47:37.693697 [debug] [Thread-2  ]: Databricks adapter: Cursor(session-id=01ef9a24-dc3e-1c09-9484-ccf75e837b1c, command-id=Unknown) - Created cursor
[0m15:47:38.353857 [debug] [Thread-2  ]: SQL status: OK in 0.990 seconds
[0m15:47:38.358351 [debug] [Thread-2  ]: Databricks adapter: Cursor(session-id=01ef9a24-dc3e-1c09-9484-ccf75e837b1c, command-id=01ef9a24-dc50-16e7-b2c8-761941eef2b0) - Closing cursor
[0m15:47:38.388535 [debug] [Thread-2  ]: Writing runtime sql for node "model.business_accounts.prepared_business_accounts"
[0m15:47:38.389369 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5364839424, session-id=01ef9a24-dc3e-1c09-9484-ccf75e837b1c, name=model.business_accounts.prepared_business_accounts, idle-time=0.6976950168609619s, acquire-count=1, language=sql, thread-identifier=(40701, 6217789440), compute-name=) - Checking idleness
[0m15:47:38.389675 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5364839424, session-id=01ef9a24-dc3e-1c09-9484-ccf75e837b1c, name=model.business_accounts.prepared_business_accounts, idle-time=0.6980240345001221s, acquire-count=1, language=sql, thread-identifier=(40701, 6217789440), compute-name=) - Retrieving connection
[0m15:47:38.389882 [debug] [Thread-2  ]: Using databricks connection "model.business_accounts.prepared_business_accounts"
[0m15:47:38.390224 [debug] [Thread-2  ]: On model.business_accounts.prepared_business_accounts: /* {"app": "dbt", "dbt_version": "1.8.7", "dbt_databricks_version": "1.8.7", "databricks_sql_connector_version": "3.1.2", "profile_name": "databricks_dbt", "target_name": "dev", "node_id": "model.business_accounts.prepared_business_accounts"} */

  
    
        create or replace table `hive_metastore`.`default`.`prepared_business_accounts`
      
      using delta
      
      
      
      
      
      
      
      as
      

with integrated_accounts as (
    select
        account_id,
        account_name,
        contact_email,
        registration_date
    from `hive_metastore`.`default`.`integrated_business_accounts`
),

transaction_metrics as (
    select
        account_id,
        count(transaction_id) as total_number_of_transactions,
        
    sum(transaction_amount)
 as total_transaction_volume,
        min(transaction_date) as first_transaction_date,
        max(transaction_date) as last_transaction_date
    from `hive_metastore`.`raw`.`raw_transactions`
    group by account_id
),

account_aggregated_data as (
    select
        ia.account_id,
        ia.account_name,
        ia.contact_email,
        ia.registration_date,
        coalesce(tm.total_number_of_transactions, 0) as total_number_of_transactions,
        coalesce(tm.total_transaction_volume, 0) as total_transaction_volume,
        tm.first_transaction_date,
        tm.last_transaction_date
    from integrated_accounts ia
    left join transaction_metrics tm on ia.account_id = tm.account_id
)

select
    account_id,
    account_name,
    contact_email,
    registration_date,
    total_number_of_transactions,
    total_transaction_volume,
    first_transaction_date,
    last_transaction_date,
    
    CAST(current_timestamp() AS TIMESTAMP) as created_at,
    CAST(current_timestamp() AS TIMESTAMP) as updated_at,
    CAST(current_timestamp() AS TIMESTAMP) as processed_at

from account_aggregated_data;
  
[0m15:47:38.390549 [debug] [Thread-2  ]: Databricks adapter: Cursor(session-id=01ef9a24-dc3e-1c09-9484-ccf75e837b1c, command-id=Unknown) - Created cursor
[0m15:47:48.402957 [debug] [Thread-2  ]: SQL status: OK in 10.010 seconds
[0m15:47:48.405292 [debug] [Thread-2  ]: Databricks adapter: Cursor(session-id=01ef9a24-dc3e-1c09-9484-ccf75e837b1c, command-id=01ef9a24-dcba-1344-95cb-755a5ca8ae88) - Closing cursor
[0m15:47:48.566281 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5364839424, session-id=01ef9a24-dc3e-1c09-9484-ccf75e837b1c, name=model.business_accounts.prepared_business_accounts, idle-time=6.198883056640625e-06s, acquire-count=0, language=sql, thread-identifier=(40701, 6217789440), compute-name=) - Released connection
[0m15:47:48.566910 [debug] [Thread-2  ]: Databricks adapter: DatabricksDBTConnection(id=5364839424, session-id=01ef9a24-dc3e-1c09-9484-ccf75e837b1c, name=model.business_accounts.prepared_business_accounts, idle-time=1.6689300537109375e-06s, acquire-count=0, language=sql, thread-identifier=(40701, 6217789440), compute-name=) - Released connection
[0m15:47:48.567378 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7197c6e1-08b7-4bdd-acfc-a3ab44159fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13fc4f190>]}
[0m15:47:48.568108 [info ] [Thread-2  ]: 3 of 3 OK created sql table model default.prepared_business_accounts ........... [[32mOK[0m in 11.23s]
[0m15:47:48.568710 [debug] [Thread-2  ]: Finished running node model.business_accounts.prepared_business_accounts
[0m15:47:48.570019 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5364503024, session-id=None, name=master, idle-time=14.4030442237854s, acquire-count=0, language=None, thread-identifier=(40701, 8635354944), compute-name=) - Checking idleness
[0m15:47:48.570462 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5364503024, session-id=None, name=master, idle-time=14.403515100479126s, acquire-count=0, language=None, thread-identifier=(40701, 8635354944), compute-name=) - Reusing connection previously named master
[0m15:47:48.570777 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5364503024, session-id=None, name=master, idle-time=14.403840065002441s, acquire-count=1, language=None, thread-identifier=(40701, 8635354944), compute-name=) - Acquired connection on thread (40701, 8635354944), using default compute resource
[0m15:47:48.571082 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5364503024, session-id=None, name=master, idle-time=14.40415620803833s, acquire-count=1, language=None, thread-identifier=(40701, 8635354944), compute-name=) - Checking idleness
[0m15:47:48.571348 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5364503024, session-id=None, name=master, idle-time=14.404419183731079s, acquire-count=1, language=None, thread-identifier=(40701, 8635354944), compute-name=) - Retrieving connection
[0m15:47:48.571596 [debug] [MainThread]: On master: ROLLBACK
[0m15:47:48.571846 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:47:48.914675 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5364503024, session-id=01ef9a24-e2f0-1262-b078-7c67ad0d3e44, name=master, idle-time=8.106231689453125e-06s, acquire-count=1, language=None, thread-identifier=(40701, 8635354944), compute-name=) - Connection created
[0m15:47:48.915606 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:47:48.916189 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5364503024, session-id=01ef9a24-e2f0-1262-b078-7c67ad0d3e44, name=master, idle-time=0.0017230510711669922s, acquire-count=1, language=None, thread-identifier=(40701, 8635354944), compute-name=) - Checking idleness
[0m15:47:48.916723 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5364503024, session-id=01ef9a24-e2f0-1262-b078-7c67ad0d3e44, name=master, idle-time=0.0022630691528320312s, acquire-count=1, language=None, thread-identifier=(40701, 8635354944), compute-name=) - Retrieving connection
[0m15:47:48.917209 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:47:48.917677 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:47:48.918183 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(id=5364503024, session-id=01ef9a24-e2f0-1262-b078-7c67ad0d3e44, name=master, idle-time=4.0531158447265625e-06s, acquire-count=0, language=None, thread-identifier=(40701, 8635354944), compute-name=) - Released connection
[0m15:47:48.918937 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:47:48.919362 [debug] [MainThread]: On master: ROLLBACK
[0m15:47:48.919729 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:47:48.920080 [debug] [MainThread]: On master: Close
[0m15:47:48.920464 [debug] [MainThread]: Databricks adapter: Connection(session-id=01ef9a24-e2f0-1262-b078-7c67ad0d3e44) - Closing connection
[0m15:47:49.031908 [debug] [MainThread]: Connection 'model.business_accounts.staging_business_accounts' was properly closed.
[0m15:47:49.033590 [debug] [MainThread]: On model.business_accounts.staging_business_accounts: ROLLBACK
[0m15:47:49.034202 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:47:49.034648 [debug] [MainThread]: On model.business_accounts.staging_business_accounts: Close
[0m15:47:49.035121 [debug] [MainThread]: Databricks adapter: Connection(session-id=01ef9a24-d3db-10cc-9342-5587f69d7b83) - Closing connection
[0m15:47:49.137061 [debug] [MainThread]: Connection 'model.business_accounts.integrated_business_accounts' was properly closed.
[0m15:47:49.137886 [debug] [MainThread]: On model.business_accounts.integrated_business_accounts: ROLLBACK
[0m15:47:49.138395 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:47:49.139070 [debug] [MainThread]: On model.business_accounts.integrated_business_accounts: Close
[0m15:47:49.139708 [debug] [MainThread]: Databricks adapter: Connection(session-id=01ef9a24-db66-16f1-b1ff-e4657fd57a32) - Closing connection
[0m15:47:49.238326 [debug] [MainThread]: Connection 'model.business_accounts.prepared_business_accounts' was properly closed.
[0m15:47:49.239841 [debug] [MainThread]: On model.business_accounts.prepared_business_accounts: ROLLBACK
[0m15:47:49.240367 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:47:49.240834 [debug] [MainThread]: On model.business_accounts.prepared_business_accounts: Close
[0m15:47:49.241586 [debug] [MainThread]: Databricks adapter: Connection(session-id=01ef9a24-dc3e-1c09-9484-ccf75e837b1c) - Closing connection
[0m15:47:49.338514 [info ] [MainThread]: 
[0m15:47:49.339178 [info ] [MainThread]: Finished running 2 view models, 1 table model in 0 hours 0 minutes and 26.18 seconds (26.18s).
[0m15:47:49.340532 [debug] [MainThread]: Command end result
[0m15:47:49.378259 [info ] [MainThread]: 
[0m15:47:49.378628 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:47:49.378858 [info ] [MainThread]: 
[0m15:47:49.379095 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m15:47:49.381516 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 28.132618, "process_user_time": 3.062966, "process_kernel_time": 2.344875, "process_mem_max_rss": "218300416", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m15:47:49.381886 [debug] [MainThread]: Command `dbt run` succeeded at 15:47:49.381823 after 28.13 seconds
[0m15:47:49.382157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103323f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13fba36a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d8e9d0>]}
[0m15:47:49.382411 [debug] [MainThread]: Flushing usage events
